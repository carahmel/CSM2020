{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define pre-processing subfunctions ####\n",
    "\n",
    "def removeNonAlphaNumChars(tweets):\n",
    "    tweets = [re.sub(r'\\-', r' ', tweet) for tweet in tweets]  # replace - with SPACE\n",
    "    tweets = [re.sub(r'[^\\w\\s]', r'', tweet) for tweet in tweets]  #remove other NON alpha numeric, excluding whitespace\n",
    "    return tweets\n",
    "\n",
    "\n",
    "# Taken from https://github.com/Deffro/text-preprocessing-techniques\n",
    "def removeUnicode(tweets):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    tweets = [re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'[^\\x00-\\x7f]',r'',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "def replaceURL(tweets):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    tweets = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "def replaceAtUser(tweets):\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    tweets = [re.sub('@[^\\s]+','atUser',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "def removeHashtagInFrontOfWord(tweets):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "#### DEFINE MAIN PREPROCESSING FUNCTIONS ####\n",
    "\n",
    "\n",
    "def preprocessTweets(tweets):\n",
    "    tweets = [tweet.lower() for tweet in tweets]  #convert to lowercase\n",
    "    tweets = replaceURL(tweets) #replace URLs\n",
    "    tweets = replaceAtUser(tweets)  # replace @user\n",
    "    tweets = removeHashtagInFrontOfWord(tweets)  # remove hashtag\n",
    "    tweets = removeNonAlphaNumChars(tweets)  # remove non alphanumeric characters\n",
    "    tweets = removeUnicode(tweets)  # remove other random unicode strings\n",
    "    return tweets\n",
    "\n",
    "def tokenize(tweets):\n",
    "    tokens = [nltk.word_tokenize(tweet) for tweet in tweets]\n",
    "    return tokens\n",
    "\n",
    "def getFinalTokens(tokens):\n",
    "    finalTokens = [[lemmatizer.lemmatize(word) for word in token if len(word) > 2 and (word not in stoplist)] for token in tokens]\n",
    "    return finalTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  DAY-7\\n\\n#lockdown in #Lagos #Abuja #Nigeria a...\n",
      "1  RT @Astartiel: Now that we know for a fact sad...\n",
      "2  RT @2013Boodicca: The first UK conviction for ...\n",
      "3  RT @PDChina: Made it! A 104-yr-old World War I...\n",
      "4  RT @DarrenPlymouth: UK #coronavirus statistics...\n",
      "\n",
      "Number of tweets:  50000\n"
     ]
    }
   ],
   "source": [
    "#### Load data ####\n",
    "\n",
    "# with open(\"covidtrack_50K.json\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.csm2020\n",
    "collection = db.assignment3\n",
    "df = pd.DataFrame(list(collection.find()))[[\"text\"]]\n",
    "print(df.head())\n",
    "\n",
    "tweets = df[\"text\"].values.tolist()\n",
    "print(\"\\nNumber of tweets: \", len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final tokens:  [['day', 'lockdown', 'lagos', 'abuja', 'nigeria', 'preventive', 'measure', 'combating', 'team', 'deci'], ['know', 'fact', 'sadly', 'crazy', 'transfer', 'even', 'asymptomatically', 'pet', 'wildlife'], ['first', 'conviction', 'breaching', 'lockdown', 'restriction', 'maria', 'dinou', 'black', 'woman', 'york', 'fined'], ['made', '104', 'old', 'world', 'war', 'veteran', 'state', 'oregon', 'become', 'oldest', 'known', 'survivor'], ['statistic', '3802', 'new', 'case', '51608', 'total', '439', 'new', 'death', '5373', 'total', 'dead', 'stay', 'safe'], ['regime', 'allocating', 'fund', 'irgc', 'instead', 'addressing', 'iran'], ['together', 'get', 'together', 'ssot', 'southsudan'], ['joke', 'candidate', 'biden', 'doesnt', 'even', 'know', 'message', 'response', 'listen', 'inter'], ['1918', 'flu', 'teach', 'economy'], ['monday', 'evening', 'update', 'european', 'call', 'galway', 'medical', 'grad', 'public', 'health', 'emergency', 'team', 'meeting', 'engaging']]\n"
     ]
    }
   ],
   "source": [
    "#### Preprocess tweets ####\n",
    "stoplist = stopwords.words('english')\n",
    "other_stopwords = \"atUser URL RT covid coronavirus covid19 covid_19 covid-19\" # I decided to include the covid terms since it's the main topic\n",
    "stoplist = stoplist + other_stopwords.split()\n",
    "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "\n",
    "tweets_clean = preprocessTweets(tweets)\n",
    "tokens = tokenize(tweets_clean)\n",
    "final_tokens = getFinalTokens(tokens)\n",
    "\n",
    "# print(\"\\nTweets: \", tweets[:10])\n",
    "# print(\"\\nCleaned tweets: \", tweets_clean[:10])\n",
    "# print(\"\\nTokens: \", tokens[:10])\n",
    "print(\"\\nFinal tokens: \", final_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topic 0 words Topic 1 words Topic 2 words Topic 3 words Topic 4 words  \\\n",
      "0         death      pandemic           new      pandemic           amp   \n",
      "1         south           lot    government          weve         would   \n",
      "2         korea       symptom           say         point           day   \n",
      "3          rate           day          case          play          help   \n",
      "4           per      fighting           job       feeling       country   \n",
      "5         china           due         death      watching        people   \n",
      "6        global          give          done       arrived          news   \n",
      "7       country         share        spread         exact           use   \n",
      "8       another      hospital      american           may          died   \n",
      "9         caput         today          poll         today           big   \n",
      "\n",
      "  Topic 5 words Topic 6 words Topic 7 words Topic 8 words Topic 9 words  \\\n",
      "0          case         press      positive         death          stay   \n",
      "1         death          live        tested        number         boris   \n",
      "2           new      pandemic    quarantine          toll       johnson   \n",
      "3         total         fight           get      breaking      minister   \n",
      "4      breaking      briefing           say      recorded       getting   \n",
      "5      reported         watch          well         total         mayor   \n",
      "6        number         every           amp      fatality         staff   \n",
      "7       highest    conference          need          past        people   \n",
      "8     confirmed         shown           see           936         prime   \n",
      "9         daily     brilliant          good         24hrs          rule   \n",
      "\n",
      "      Topic 10 words Topic 11 words Topic 12 words Topic 13 words  \\\n",
      "0               time        testing         medium         health   \n",
      "1           lockdown           home       briefing         spread   \n",
      "2               life         spread          video         public   \n",
      "3               week           stop           help        protect   \n",
      "4               http           many         called          india   \n",
      "5  lockdownextension       stayhome       research           home   \n",
      "6              whole       pakistan    restriction          world   \n",
      "7            imagine         coming        grocery       pandemic   \n",
      "8     quarantinelife       pandemic           tell           news   \n",
      "9              china         family         worker          fight   \n",
      "\n",
      "  Topic 14 words Topic 15 words Topic 16 words Topic 17 words Topic 18 words  \\\n",
      "0       outbreak         crisis            via           test         latest   \n",
      "1            way         people           keep          china         thanks   \n",
      "2           week            yet           safe         corona           news   \n",
      "3            new      continues             nh           work          daily   \n",
      "4            usa        current            ppe         people       business   \n",
      "5           york      suffering           govt           mask          state   \n",
      "6            old     accelerate        provide           find      emergency   \n",
      "7        looking    drastically      frontline       lockdown            man   \n",
      "8         people       occupant           sign            amp            amp   \n",
      "9           amid            whi       petition        million          japan   \n",
      "\n",
      "         Topic 19 words  \n",
      "0                 trump  \n",
      "1             president  \n",
      "2                crisis  \n",
      "3               medical  \n",
      "4                advice  \n",
      "5                  want  \n",
      "6                latest  \n",
      "7                  dont  \n",
      "8  trumppressconference  \n",
      "9                warned  \n"
     ]
    }
   ],
   "source": [
    "#### Perform LDA ####\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy,max_df=0.9, min_df=10)\n",
    "\n",
    "tfreq = vectorizer.fit_transform(final_tokens)\n",
    "tfreq_fNames = vectorizer.get_feature_names()\n",
    "# print(tfreq_fNames)\n",
    "\n",
    "nTopics = 20\n",
    "model = LatentDirichletAllocation(n_components=nTopics, random_state=0)\n",
    "model.fit(tfreq)\n",
    "\n",
    "def display_topic_names(model, feature_names, no_top_words):  # adapted from https://ourcodingclub.github.io/tutorials/topic-modelling-python/\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "# def display_topic_weights(model, feature_names, no_top_words):  # adapted from https://ourcodingclub.github.io/tutorials/topic-modelling-python/\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "nTopWords = 10\n",
    "# topicWords = display_topics(model, tfreq_fNames, nTopWords)\n",
    "topicWords = display_topic_names(model, tfreq_fNames, nTopWords)\n",
    "# topicWeights = display_topic_weights(model, tfreq_fNames, nTopWords)\n",
    "# topicWords.to_csv(\"words_perTopic.csv\", encoding = \"utf-8\")\n",
    "print(topicWords)\n",
    "# print(\"\\n\\n\", topicWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected topics (3), semantic names, and top 5 words:\n",
    "\n",
    "- Topic 9: __Situation in the UK__ (stay, boris, johnson, minister, getting)\n",
    "- Topic 8: __News headlines__ (death, number, toll, breaking, recorded)\n",
    "- Topic 6: __Media/press communications__ (press, live, pandemic, fight, briefing)\n",
    "\n",
    "## Discussion:\n",
    "None of the 20 topics stood out as being particularly surprising, but it was quite difficult to interpret and characterise many of the topics. First, I felt that some topics were not \"distinct\" (i.e. overlapped with one or more other topics). For example, Topics 5 and 8 are similar and could be characterised as \"Headlines\". Similarly, Topics 6 and 12 are also comparable with both touching on the subject of media/press communications. Overall, a few key ideas could be inferred from the 20 topics, but I would say assuming that 20 different latent topics exist might be abit of a reach. Moreover, all the tweets were pre-filtered and talk about the same global topic: Covid-19. Thus, compared to the examples showed in the lecture slides, the \"quality\" of each topic identified via LDA is not at the same level. \n",
    "\n",
    "I found that topics relating to specific geographic locations were the clearest. For example, Topics 0, 9, and 19 quite clearly suggest Covid-19 subtopics that pertain specifically to South Korea, the UK, and the US respectively. \n",
    "\n",
    "Finally, there were also a few topics (e.g. Topics 3, 4, 10, and 16) that contained words that I could not fully comprehend (e.g. weve, amp, http, nh). Clearly, \"http\" comes from url strings in the tweets; but given the pre-processing procedure that removed URLs, I was suprised to see that \"http\" still appeared as a top keyword and I can't explain this occurrence...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
