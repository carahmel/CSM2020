{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tweets = []\n",
    "tmin = 500\n",
    "tmax = 479500 # latest file name\n",
    "\n",
    "for i in range(tmin, tmax + 1, 500):\n",
    "    file_name = \"../dataKA/all_tweets/tweets\" + str(i)\n",
    "    file_object = open(file_name, 'rb')\n",
    "    for tweet in pickle.load(file_object):\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"covid\", \"covid19\", \"covid2019\", \"covid-19\", \"covid-2019\", \"covid_19\", \"covid_2019\", \"COVIDー19\",\n",
    "            \"COVIDー2019\", \"corona\", \"virus\", \"coronavirus\", \"coronakrise\", \"coronacrisis\", \"corona-crise\", \n",
    "            \"pandemic\", \"coronapandemic\", \"coronavirusapandemic\", \"covidpandemic\", \"covid19pandemic\", \"covid2019pandemic\" \n",
    "            \"iorestoacasa\", \"forzalombardia\", \"quarantena\", \"covidswitzerland\", \"wtfockdown\", \"stayhome\", \"stayathome\",\n",
    "            \"staysafe\", \"stayhomesavelives\", \"stayhomestaysafe\", \"lockdown\", \"socialdistancing\", \"distancing\", \"quarantine\",\n",
    "            \"quarantinelife\", \"confinement\", \"confinementjour\", \"restezchezvous\", \"lavuedepuismonconfinement\", \n",
    "            \"BloqueoNoSolidaridadSi\", \"protectyourselfandothers\", \"coronavirusdelcastigominore\", \"coronainfoch\", \"masken\",\n",
    "            \"masques\", \"masks\", \"homeoffice\", \"wfh\", \"workfromhome\", \"BAG_OFSP_UFSP\", \"restriktionen\"] \n",
    "\n",
    "\n",
    "# from Karolos\n",
    "def check(words, sentence):\n",
    "    for word in words:\n",
    "        if word in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "keywords_lower = [k.lower() for k in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. IT covid tweets:  736\n"
     ]
    }
   ],
   "source": [
    "# COVID-related\n",
    "it_cov_date = []\n",
    "it_cov_tid = []\n",
    "it_cov_text = []\n",
    "it_cov_tweets = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    if check(keywords_lower, tweet[\"text\"].lower()):\n",
    "        if tweet[\"lang\"] == \"it\":\n",
    "            it_cov_tweets.append(tweet)\n",
    "            it_cov_tid.append(tweet[\"id\"])\n",
    "            it_cov_date.append(tweet[\"created_at\"])\n",
    "            try:\n",
    "                it_cov_text.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            except KeyError:\n",
    "                it_cov_text.append(tweet[\"text\"])\n",
    "            \n",
    "    elif \"extended_tweet\" in tweet and check(keywords_lower, tweet[\"extended_tweet\"][\"full_text\"]):\n",
    "        if tweet[\"lang\"] == \"it\":\n",
    "            cnt = cnt + 1\n",
    "            it_cov_tweets.append(tweet)\n",
    "            it_cov_tid.append(tweet[\"id\"])\n",
    "            it_cov_date.append(tweet[\"created_at\"])\n",
    "            it_cov_text.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            \n",
    "    elif \"retweeted_status\" in tweet and check(keywords_lower, tweet[\"retweeted_status\"][\"full_text\"]):\n",
    "        if tweet[\"lang\"] == \"it\":\n",
    "            cnt = cnt + 1\n",
    "            it_cov_tweets.append(tweet)\n",
    "            it_cov_tid.append(tweet[\"id\"])\n",
    "            it_cov_date.append(tweet[\"created_at\"])\n",
    "            it_cov_text.append(tweet[\"retweeted_status\"][\"full_text\"])\n",
    "            \n",
    "    elif \"quoted_status\" in tweet and check(keywords_lower, tweet[\"quoted_status\"][\"text\"]):\n",
    "        if tweet[\"lang\"] == \"it\":\n",
    "            cnt = cnt + 1\n",
    "            it_cov_tweets.append(tweet)\n",
    "            it_cov_tid.append(tweet[\"id\"])\n",
    "            it_cov_date.append(tweet[\"created_at\"])\n",
    "            it_cov_text.append(tweet[\"quoted_status\"][\"text\"])\n",
    "\n",
    "print(\"No. IT covid tweets: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. IT tweets (total):  43481\n"
     ]
    }
   ],
   "source": [
    "# All IT tweets\n",
    "it_all_date = []\n",
    "it_all_tid = []\n",
    "it_all_text = []\n",
    "it_all_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    if tweet['lang'] == 'it':\n",
    "        it_all_tweets.append(tweet)\n",
    "        it_all_tid.append(tweet[\"id\"])\n",
    "        it_all_date.append(tweet[\"created_at\"])\n",
    "        try:\n",
    "            it_all_text.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "        except KeyError:\n",
    "            it_all_text.append(tweet[\"text\"])\n",
    "\n",
    "print(\"No. IT tweets (total): \", len(it_all_tid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## Export full tweet datasets ##\n",
    "\n",
    "it_all = pd.concat([pd.Series(it_all_tid), \n",
    "                           pd.Series(it_all_date), \n",
    "                           pd.Series(it_all_text)],\n",
    "                          axis = 1)\n",
    "it_covid = pd.concat([pd.Series(it_cov_tid),\n",
    "                             pd.Series(it_cov_date),\n",
    "                             pd.Series(it_cov_text)],\n",
    "                            axis = 1)\n",
    "\n",
    "# it_all.to_csv(\"../dataKA/latest/it_tweets_all.csv\", encoding = \"utf-8\")\n",
    "# it_covid.to_csv(\"../dataKA/latest/it_tweets_covid.csv\", encoding = \"utf-8\")\n",
    "\n",
    "## Export text of covid dataset and full dataset for tool training ## \n",
    "out_it_all_text = pd.DataFrame(it_all_text)\n",
    "out_it_cov_text = pd.DataFrame(it_cov_text)\n",
    "\n",
    "# out_it_all_text.to_csv(\"../dataKA/latest/it_alltext.csv\", encoding = \"utf-8\")\n",
    "# out_it_cov_text.to_csv(\"../dataKA/latest/it_covidtext.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#### Define pre-processing subfunctions ####\n",
    "\n",
    "def removeNonAlphaNumChars(tweets):\n",
    "    tweets = [re.sub(r'\\-', r' ', tweet) for tweet in tweets]  # replace - with SPACE\n",
    "    tweets = [re.sub(r'[^\\w\\s]', r'', tweet) for tweet in tweets]  #remove other NON alpha numeric, excluding whitespace\n",
    "    return tweets\n",
    "\n",
    "def replaceContractions(tweets):\n",
    "    tweets = [re.sub(r\"[’\\']\", r' ', tweet) for tweet in tweets]  ## !!!! WEIRD APOSTROPHE BEWARE !!!\n",
    "    return tweets\n",
    "\n",
    "# FROM DEFFRO: https://github.com/Deffro/text-preprocessing-techniques\n",
    "def removeUnicode(tweets):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    tweets = [re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'[^\\x00-\\x7f]',r'',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceURL(tweets):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    tweets = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceAtUser(tweets):\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    tweets = [re.sub('@[^\\s]+','atUser',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def removeHashtagInFrontOfWord(tweets):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "# end DEFFRO\n",
    "\n",
    "\n",
    "#### DEFINE MAIN PREPROCESSING FUNCTIONS ####\n",
    "\n",
    "def cleanTweets(tweets):\n",
    "    tweets = [tweet.lower() for tweet in tweets]  #convert to lowercase\n",
    "    tweets = replaceURL(tweets) #replace URLs\n",
    "    tweets = replaceAtUser(tweets)  # replace @user\n",
    "    tweets = removeHashtagInFrontOfWord(tweets)  # remove hashtag\n",
    "    tweets = replaceContractions(tweets) # replace ' contractions with space (e.g. l'ordre = l ordre)\n",
    "    tweets = removeNonAlphaNumChars(tweets)  # remove non alphanumeric characters\n",
    "    return tweets\n",
    "\n",
    "def tokenizeTweets(tweets):\n",
    "    tokens = [nltk.word_tokenize(tweet) for tweet in tweets]\n",
    "    return tokens\n",
    "\n",
    "def applyStoplist(tokens):\n",
    "    tokens = [[word for word in token if len(word) > 2 and (word not in stoplist)] for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stemTokens(tokens):\n",
    "    tokens = [[stemmer.stem(word) for word in token] for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral:  20000\n",
      "No. positive:  23552\n",
      "No. negative:  6542\n"
     ]
    }
   ],
   "source": [
    "# Import preclassified IT tweets (football)\n",
    "it_preClass = pd.read_csv(\"C:/Users/pang/Desktop/Stuff/_PhD/PhD Courses/_CSM/Project/preClassified/IT_3cat_worldcup.csv\")\n",
    "# it_preClass = shuffle(it_preClass)\n",
    "\n",
    "neutral = it_preClass[it_preClass[\"sentiment\"]==\"NEUTRAL\"]\n",
    "positive = it_preClass[it_preClass[\"sentiment\"]==\"POSITIVE\"]\n",
    "negative = it_preClass[it_preClass[\"sentiment\"]==\"NEGATIVE\"]\n",
    "\n",
    "neutral = shuffle(neutral)[:20000]\n",
    "# positive = shuffle(positive)[:20000]\n",
    "# negative = shuffle(negative)[:20000]\n",
    "\n",
    "print(\"No. neutral: \", len(neutral))\n",
    "print(\"No. positive: \", len(positive))\n",
    "print(\"No. negative: \", len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start tweet preprocessing ##\n",
    "stoplist = stopwords.words('italian')\n",
    "other_stopwords = \"atUser URL\"\n",
    "stoplist = stoplist + other_stopwords.split() # final list of stopwords\n",
    "# lemmatizer = WordNetLemmatizer() # only for EN\n",
    "stemmer = SnowballStemmer('italian') # set stemmer\n",
    "\n",
    "it_preClass_pos_clean = cleanTweets(positive[\"tweet_text\"])\n",
    "it_preClass_pos_tokens = tokenizeTweets(it_preClass_pos_clean)\n",
    "it_preClass_pos_tokens = applyStoplist(it_preClass_pos_tokens)\n",
    "it_preClass_pos_tokens = stemTokens(it_preClass_pos_tokens)\n",
    "\n",
    "it_preClass_neg_clean = cleanTweets(negative[\"tweet_text\"])\n",
    "it_preClass_neg_tokens = tokenizeTweets(it_preClass_neg_clean)\n",
    "it_preClass_neg_tokens = applyStoplist(it_preClass_neg_tokens)\n",
    "it_preClass_neg_tokens = stemTokens(it_preClass_neg_tokens)\n",
    "\n",
    "it_preClass_neu_clean = cleanTweets(neutral[\"tweet_text\"])\n",
    "it_preClass_neu_tokens = tokenizeTweets(it_preClass_neu_clean)\n",
    "it_preClass_neu_tokens = applyStoplist(it_preClass_neu_tokens)\n",
    "it_preClass_neu_tokens = stemTokens(it_preClass_neu_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_pos_labeled = [[token, \"Positive\"] for token in it_preClass_pos_tokens]\n",
    "it_neg_labeled = [[token, \"Negative\"] for token in it_preClass_neg_tokens]\n",
    "it_neu_labeled = [[token, \"Neutral\"] for token in it_preClass_neu_tokens]\n",
    "\n",
    "it_dataset = it_pos_labeled + it_neg_labeled + it_neu_labeled\n",
    "it_dataset = shuffle(pd.DataFrame(it_dataset))\n",
    "\n",
    "# split features and labels for SKLEARN classifier\n",
    "features = it_dataset.loc[:, 0]\n",
    "labels  = it_dataset.loc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in-domain test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for classifier training (SKLEARN) ##\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=dummy,preprocessor=dummy,max_df=0.9, min_df=10)\n",
    "X=cv.fit_transform(features).toarray()\n",
    "y=labels\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAIVE BAYES USING SKLEARN ##\n",
    "\n",
    "#Import Multinomial Naive Bayes model (https://towardsdatascience.com/sentiment-analysis-of-tweets-using-multinomial-naive-bayes-1009ed24276b)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a multinomial NB Classifier\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_NB = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## RANDOM FOREST USING SKLEARN ##\n",
    "\n",
    "# # Fitting classifier to the Training set\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# RFclassifier=RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "# RFclassifier.fit(X_train,y_train)\n",
    "# # Predicting the Test set results\n",
    "# y_pred_RF = RFclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.60      0.60      0.60      1983\n",
      "     Neutral       0.66      0.59      0.62      5951\n",
      "    Positive       0.73      0.79      0.76      7095\n",
      "\n",
      "    accuracy                           0.69     15029\n",
      "   macro avg       0.66      0.66      0.66     15029\n",
      "weighted avg       0.69      0.69      0.69     15029\n",
      "\n",
      "\n",
      "Accuracy (NB): 0.6888681881695389\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y_test, y_pred_NB))\n",
    "print(\"\\nAccuracy (NB):\", metrics.accuracy_score(y_test, y_pred_NB))\n",
    "\n",
    "# print (\"\\n\", classification_report(y_test, y_pred_RF))\n",
    "# print(\"\\nAccuracy (RF):\", metrics.accuracy_score(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use collected tweets as test set instead of in-domain tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### load manually classified tweets (CH/Covid) ###\n",
    "\n",
    "# it_labeled_all = pd.read_csv(\"../dataKA/frlabeledAll1.csv\")\n",
    "# it_labeled_cov = pd.read_csv(\"../dataKA/frlabeledCovid1.csv\")\n",
    "\n",
    "# ## ALL ##\n",
    "# neu = it_labeled_all[it_labeled_all[\"label\"]==\"neu\"]\n",
    "# pos = it_labeled_all[it_labeled_all[\"label\"]==\"pos\"]\n",
    "# neg = it_labeled_all[it_labeled_all[\"label\"]==\"neg\"]\n",
    "# print(\"No. neutral (all): \", len(neu))\n",
    "# print(\"No. positive (all): \", len(pos))\n",
    "# print(\"No. negative (all): \", len(neg))\n",
    "\n",
    "# # Start preprocessing #\n",
    "# it_labeled_all_pos_clean = cleanTweets(pos[\"text\"])\n",
    "# it_labeled_all_pos_tokens = tokenizeTweets(it_labeled_all_pos_clean)\n",
    "# it_labeled_all_pos_tokens = applyStoplist(it_labeled_all_pos_tokens)\n",
    "# it_labeled_all_pos_tokens = stemTokens(it_labeled_all_pos_tokens)\n",
    "\n",
    "# it_labeled_all_neg_clean = cleanTweets(neg[\"text\"])\n",
    "# it_labeled_all_neg_tokens = tokenizeTweets(it_labeled_all_neg_clean)\n",
    "# it_labeled_all_neg_tokens = applyStoplist(it_labeled_all_neg_tokens)\n",
    "# it_labeled_all_neg_tokens = stemTokens(it_labeled_all_neg_tokens)\n",
    "\n",
    "# it_labeled_all_neu_clean = cleanTweets(neu[\"text\"])\n",
    "# it_labeled_all_neu_tokens = tokenizeTweets(it_labeled_all_neu_clean)\n",
    "# it_labeled_all_neu_tokens = applyStoplist(it_labeled_all_neu_tokens)\n",
    "# it_labeled_all_neu_tokens = stemTokens(it_labeled_all_neu_tokens)\n",
    "\n",
    "# it_pos_labeled_all = [[token, \"Positive\"] for token in it_labeled_all_pos_tokens]\n",
    "# it_neg_labeled_all = [[token, \"Negative\"] for token in it_labeled_all_neg_tokens]\n",
    "# it_neu_labeled_all = [[token, \"Neutral\"] for token in it_labeled_all_neu_tokens]\n",
    "\n",
    "# it_dataset_all = it_pos_labeled_all + it_neg_labeled_all + it_neu_labeled_all\n",
    "# it_dataset_all = shuffle(pd.DataFrame(it_dataset_all))\n",
    "\n",
    "\n",
    "# ## COVID ##\n",
    "# neu_cov = it_labeled_cov[it_labeled_cov[\"label\"]==\"neu\"]\n",
    "# pos_cov = it_labeled_cov[it_labeled_cov[\"label\"]==\"pos\"]\n",
    "# neg_cov = it_labeled_cov[it_labeled_cov[\"label\"]==\"neg\"]\n",
    "# print(\"No. neutral (covid): \", len(neu_cov))\n",
    "# print(\"No. positive (covid): \", len(pos_cov))\n",
    "# print(\"No. negative (covid): \", len(neg_cov))\n",
    "\n",
    "# ## Start preprocessing ##\n",
    "# it_labeled_cov_pos_clean = cleanTweets(pos_cov[\"text\"])\n",
    "# it_labeled_cov_pos_tokens = tokenizeTweets(it_labeled_cov_pos_clean)\n",
    "# it_labeled_cov_pos_tokens = applyStoplist(it_labeled_cov_pos_tokens)\n",
    "# it_labeled_cov_pos_tokens = stemTokens(it_labeled_cov_pos_tokens)\n",
    "\n",
    "# it_labeled_cov_neg_clean = cleanTweets(neg_cov[\"text\"])\n",
    "# it_labeled_cov_neg_tokens = tokenizeTweets(it_labeled_cov_neg_clean)\n",
    "# it_labeled_cov_neg_tokens = applyStoplist(it_labeled_cov_neg_tokens)\n",
    "# it_labeled_cov_neg_tokens = stemTokens(it_labeled_cov_neg_tokens)\n",
    "\n",
    "# it_labeled_cov_neu_clean = cleanTweets(neu_cov[\"text\"])\n",
    "# it_labeled_cov_neu_tokens = tokenizeTweets(it_labeled_cov_neu_clean)\n",
    "# it_labeled_cov_neu_tokens = applyStoplist(it_labeled_cov_neu_tokens)\n",
    "# it_labeled_cov_neu_tokens = stemTokens(it_labeled_cov_neu_tokens)\n",
    "\n",
    "# it_pos_labeled_cov = [[token, \"Positive\"] for token in it_labeled_cov_pos_tokens]\n",
    "# it_neg_labeled_cov = [[token, \"Negative\"] for token in it_labeled_cov_neg_tokens]\n",
    "# it_neu_labeled_cov = [[token, \"Neutral\"] for token in it_labeled_cov_neu_tokens]\n",
    "\n",
    "# it_dataset_cov = it_pos_labeled_cov + it_neg_labeled_cov + it_neu_labeled_cov\n",
    "# it_dataset_cov = shuffle(pd.DataFrame(it_dataset_cov))\n",
    "\n",
    "\n",
    "# ## split features and labels for SKLEARN classifier\n",
    "# features_all = fr_dataset_all.loc[:, 0]\n",
    "# labels_all  = fr_dataset_all.loc[:, 1]\n",
    "# features_cov = fr_dataset_cov.loc[:, 0]\n",
    "# labels_cov  = fr_dataset_cov.loc[:, 1]\n",
    "\n",
    "# features1 = pd.concat([features_all, features], axis=0)\n",
    "# labels1  = pd.concat([labels_all, labels], axis=0)\n",
    "\n",
    "# features2 = pd.concat([features_cov, features], axis=0)\n",
    "# labels2  = pd.concat([labels_cov, labels], axis=0)\n",
    "\n",
    "# print(\"No. features (preclassified): \", len(features))\n",
    "# print(\"No. features (all): \", len(features1)-len(features))\n",
    "# print(\"No. features (covid): \", len(features2)-len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use manually labelled tweets (ALL) only as test set ##\n",
    "\n",
    "# X1=cv.fit_transform(features1).toarray()\n",
    "# y1=labels1\n",
    "# # X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 249, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "# test_all_index = 500 # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT (first 500 are collected tweets)\n",
    "# X_test1, X_train1 = X1[:test_all_index], X1[test_all_index:]\n",
    "# y_test1, y_train1 = y1[:test_all_index], y1[test_all_index:]\n",
    "# print(\"Size (test set): \", len(X_test1))\n",
    "\n",
    "# # sklearn multinomial NB classifier #\n",
    "# mnb.fit(X_train1, y_train1)\n",
    "# y_pred_NB1 = mnb.predict(X_test1)\n",
    "# print (\"\\n\", classification_report(y_test1, y_pred_NB1))\n",
    "# print(\"\\nAccuracy (NB) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_NB1))\n",
    "\n",
    "# # sklearn random forest classfier #\n",
    "# RFclassifier.fit(X_train1,y_train1)\n",
    "# y_pred_RF1 = RFclassifier.predict(X_test1)\n",
    "# print (\"\\n\", classification_report(y_test1, y_pred_RF1))\n",
    "# print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_RF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use manually labelled tweets (COVID) only as test set ##\n",
    "\n",
    "# X2=cv.fit_transform(features2).toarray()\n",
    "# y2=labels2\n",
    "# # X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 250, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "# test_cov_index = 500 # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "# X_test2, X_train2 = X2[:test_cov_index], X2[test_cov_index:]\n",
    "# y_test2, y_train2 = y2[:test_cov_index], y2[test_cov_index:]\n",
    "# print(\"Size (test set): \", len(X_test2))\n",
    "# print(y_test2[:3])\n",
    "\n",
    "# # sklearn multinomial NB classifier #\n",
    "# mnb.fit(X_train2, y_train2)\n",
    "# y_pred_NB2 = mnb.predict(X_test2)\n",
    "# print (\"\\n\", classification_report(y_test2, y_pred_NB2))\n",
    "# print(\"\\nAccuracy (NB) of manually classified tweets, all:\", metrics.accuracy_score(y_test2, y_pred_NB2))\n",
    "\n",
    "# # sklearn random forest classfier #\n",
    "# RFclassifier.fit(X_train2, y_train2)\n",
    "# y_pred_RF2 = RFclassifier.predict(X_test2)\n",
    "# print (\"\\n\", classification_report(y_test2, y_pred_RF2))\n",
    "# print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test2, y_pred_RF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply trained tool to predict polarity of collected IT tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  43481\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn mNB classifier\n",
    "labels_all = pd.Series([\"tbc\"] * len(it_all_tweets))\n",
    "features_all = cleanTweets(it_all_text)\n",
    "features_all = tokenizeTweets(features_all)\n",
    "features_all = applyStoplist(features_all)\n",
    "features_all = stemTokens(features_all)\n",
    "features_all = pd.Series(features_all)\n",
    "labels_train = labels\n",
    "features_train = features\n",
    "\n",
    "X_all = cv.fit_transform(pd.concat([features_train, features_all], axis=0)).toarray()\n",
    "y_all = pd.concat([labels_train, labels_train], axis=0)\n",
    "\n",
    "cv.fit_transform(features).toarray()\n",
    "train_index = len(features_train) # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_all_train, X_all_test = X_all[:train_index], X_all[train_index:]\n",
    "y_all_train, y_all_test = y_all[:train_index], y_all[train_index:]\n",
    "\n",
    "print(\"Size (test set): \", len(X_all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. predictions:  43481\n",
      "['Positive' 'Positive' 'Neutral' 'Positive' 'Neutral' 'Negative'\n",
      " 'Negative' 'Neutral' 'Neutral' 'Positive']\n",
      "                     0                               1  \\\n",
      "0  1244239906630774791  Sun Mar 29 12:28:05 +0000 2020   \n",
      "1  1244240206485741568  Sun Mar 29 12:29:16 +0000 2020   \n",
      "2  1244240370415931392  Sun Mar 29 12:29:55 +0000 2020   \n",
      "3  1244240702164340736  Sun Mar 29 12:31:14 +0000 2020   \n",
      "4  1244241096156332038  Sun Mar 29 12:32:48 +0000 2020   \n",
      "\n",
      "                                                   2         3  \n",
      "0                             @LordOfVenice Forza...  Positive  \n",
      "1                    @Crocket81893187 :-( \\nForza...  Positive  \n",
      "2  Tra i pochi campionati ancora attivi, quello d...   Neutral  \n",
      "3  Desideriamo offire il nostro contributo concre...  Positive  \n",
      "4  Aspettando che tutto torni come prima anche se...   Neutral  \n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "mnb.fit(X_all_train, y_all_train)\n",
    "y_all_pred_NB = mnb.predict(X_all_test)\n",
    "it_res_all_NB = pd.concat([pd.Series(it_all_tid), pd.Series(it_all_date),\n",
    "                     pd.Series(it_all_text), pd.Series(y_all_pred_NB)], \n",
    "                    axis = 1) \n",
    "it_res_all_NB.to_csv(\"../dataKA/latest/it_res_all_nb.csv\", encoding = \"utf-8\")\n",
    "print(\"No. predictions: \", len(y_all_pred_NB))\n",
    "print(y_all_pred_NB[:10])\n",
    "print(it_res_all_NB[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random forest\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# RFclassifier=RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "# RFclassifier.fit(X_all_train, y_all_train)\n",
    "# y_all_pred_RF2 = RFclassifier.predict(X_all_test)\n",
    "# res_all_RF = pd.concat([pd.Series(fr_all_tweet_id), pd.Series(fr_all_date), \n",
    "#                      pd.Series(fr_all_coord), pd.Series(fr_all_place), \n",
    "#                      pd.Series(fr_all_text), pd.Series(y_all_pred_RF2)], \n",
    "#                     axis = 1) \n",
    "# res_all_RF.to_csv(\"../dataKA/res_all_FR_rf.csv\", encoding = \"utf-8\")\n",
    "# print(\"No. predictions: \", len(y_all_pred_RF2))\n",
    "# print(res_all_RF[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
