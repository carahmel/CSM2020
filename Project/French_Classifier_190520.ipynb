{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tweets = []\n",
    "tmin = 500\n",
    "tmax = 334000 # latest file name\n",
    "\n",
    "for i in range(tmin, tmax + 1, 500):\n",
    "    file_name = \"../dataKA/all_tweets_may11/tweets\" + str(i)\n",
    "    file_object = open(file_name, 'rb')\n",
    "    for tweet in pickle.load(file_object):\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% relevant tweets (total):  6.76437125748503\n",
      "% FR tweets (relevant):  28.106050546629486\n",
      "No. FR covid tweets:  6350\n",
      "% EN tweets (relevant):  29.062098880184127\n",
      "No. EN covid tweets:  6566\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"covid\", \"covid19\", \"covid2019\", \"covid-19\", \"covid-2019\", \"covid_19\", \"covid_2019\", \"COVIDãƒ¼19\",\n",
    "            \"COVIDãƒ¼2019\", \"corona\", \"virus\", \"coronavirus\", \"coronakrise\", \"coronacrisis\", \"corona-crise\", \n",
    "            \"pandemic\", \"coronapandemic\", \"coronavirusapandemic\", \"covidpandemic\", \"covid19pandemic\", \"covid2019pandemic\" \n",
    "            \"iorestoacasa\", \"forzalombardia\", \"quarantena\", \"covidswitzerland\", \"wtfockdown\", \"stayhome\", \"stayathome\",\n",
    "            \"staysafe\", \"stayhomesavelives\", \"stayhomestaysafe\", \"lockdown\", \"socialdistancing\", \"distancing\", \"quarantine\",\n",
    "            \"quarantinelife\", \"confinement\", \"confinementjour\", \"restezchezvous\", \"lavuedepuismonconfinement\", \n",
    "            \"BloqueoNoSolidaridadSi\", \"protectyourselfandothers\", \"coronavirusdelcastigominore\", \"coronainfoch\", \"masken\",\n",
    "            \"masques\", \"masks\", \"homeoffice\", \"wfh\", \"workfromhome\", \"BAG_OFSP_UFSP\", \"restriktionen\"] \n",
    "\n",
    "\n",
    "# from Karolos\n",
    "def check(words, sentence):\n",
    "    for word in words:\n",
    "        if word in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "keywords_lower = [k.lower() for k in keywords]\n",
    "\n",
    "relevant_tweets = []\n",
    "languages = []\n",
    "\n",
    "fr_date = []\n",
    "fr_tweet_id = []\n",
    "fr_coord = []\n",
    "fr_place = []\n",
    "fr_tweets = []\n",
    "fr_covid = []\n",
    "\n",
    "en_tweets = []\n",
    "en_covid = []\n",
    "\n",
    "cnt = 0\n",
    "cnt_fr = 0\n",
    "cnt_en = 0\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    if check(keywords_lower, tweet[\"text\"].lower()):\n",
    "        cnt = cnt + 1\n",
    "        languages.append(tweet[\"lang\"])\n",
    "        relevant_tweets.append(tweet)\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt_fr = cnt_fr + 1\n",
    "            fr_tweets.append(tweet)\n",
    "            fr_covid.append(tweet[\"text\"])\n",
    "            fr_tweet_id.append(tweet[\"id\"])\n",
    "            fr_date.append(tweet[\"created_at\"])\n",
    "            fr_coord.append(tweet[\"place\"][\"bounding_box\"][\"coordinates\"])\n",
    "            fr_place.append(tweet[\"place\"][\"name\"])\n",
    "        elif tweet[\"lang\"] == \"en\":\n",
    "            cnt_en = cnt_en + 1\n",
    "            en_tweets.append(tweet)\n",
    "            en_covid.append(tweet[\"text\"])\n",
    "            \n",
    "    elif \"extended_tweet\" in tweet and check(keywords_lower, tweet[\"extended_tweet\"][\"full_text\"]):\n",
    "        cnt = cnt + 1\n",
    "        languages.append(tweet[\"lang\"])\n",
    "        relevant_tweets.append(tweet)\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt_fr = cnt_fr + 1\n",
    "            fr_tweets.append(tweet)\n",
    "            fr_covid.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            fr_tweet_id.append(tweet[\"id\"])\n",
    "            fr_date.append(tweet[\"created_at\"])\n",
    "            fr_coord.append(tweet[\"place\"][\"bounding_box\"][\"coordinates\"])\n",
    "            fr_place.append(tweet[\"place\"][\"name\"])\n",
    "        elif tweet[\"lang\"] == \"en\":\n",
    "            cnt_en = cnt_en + 1\n",
    "            en_tweets.append(tweet) \n",
    "            en_covid.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            \n",
    "    elif \"retweeted_status\" in tweet and check(keywords_lower, tweet[\"retweeted_status\"][\"full_text\"]):\n",
    "        cnt = cnt + 1  \n",
    "        languages.append(tweet[\"lang\"])\n",
    "        relevant_tweets.append(tweet)\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt_fr = cnt_fr + 1\n",
    "            fr_tweets.append(tweet) \n",
    "            fr_covid.append(tweet[\"retweeted_status\"][\"full_text\"])\n",
    "            fr_tweet_id.append(tweet[\"id\"])\n",
    "            fr_date.append(tweet[\"created_at\"])\n",
    "            fr_coord.append(tweet[\"place\"][\"bounding_box\"][\"coordinates\"])\n",
    "            fr_place.append(tweet[\"place\"][\"name\"])\n",
    "        elif tweet[\"lang\"] == \"en\":\n",
    "            cnt_en = cnt_en + 1\n",
    "            en_tweets.append(tweet) \n",
    "            en_covid.append(tweet[\"retweeted_status\"][\"full_text\"])\n",
    "            \n",
    "    elif \"quoted_status\" in tweet and check(keywords_lower, tweet[\"quoted_status\"][\"text\"]):\n",
    "        cnt = cnt + 1\n",
    "        languages.append(tweet[\"lang\"])\n",
    "        relevant_tweets.append(tweet)\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt_fr = cnt_fr + 1\n",
    "            fr_tweets.append(tweet)\n",
    "            fr_covid.append(tweet[\"quoted_status\"][\"text\"])\n",
    "            fr_tweet_id.append(tweet[\"id\"])\n",
    "            fr_date.append(tweet[\"created_at\"])\n",
    "            fr_coord.append(tweet[\"place\"][\"bounding_box\"][\"coordinates\"])\n",
    "            fr_place.append(tweet[\"place\"][\"name\"])\n",
    "        elif tweet[\"lang\"] == \"en\":\n",
    "            cnt_en = cnt_en + 1\n",
    "            en_tweets.append(tweet) \n",
    "            en_covid.append(tweet[\"quoted_status\"][\"text\"])\n",
    "            \n",
    "print(\"% relevant tweets (total): \", float(cnt) / len(tweets) * 100)\n",
    "print(\"% FR tweets (relevant): \", float(cnt_fr) / len(relevant_tweets) * 100)\n",
    "print(\"No. FR covid tweets: \", cnt_fr)\n",
    "print(\"% EN tweets (relevant): \", float(cnt_en) / len(relevant_tweets) * 100)\n",
    "print(\"No. EN covid tweets: \", cnt_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. FR tweets (total):  77294\n"
     ]
    }
   ],
   "source": [
    "fr_all_text = []\n",
    "fr_all_date = []\n",
    "fr_all_tweet_id = []\n",
    "fr_all_coord = []\n",
    "fr_all_place = []\n",
    "fr_all_tweets = []\n",
    "fr_all_covid = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    if tweet['lang'] == 'fr':\n",
    "        #fr_all_tweets.append(tweet)\n",
    "        fr_all_text.append(tweet[\"text\"])\n",
    "        fr_all_tweet_id.append(tweet[\"id\"])\n",
    "        fr_all_date.append(tweet[\"created_at\"])\n",
    "        fr_all_coord.append(tweet[\"place\"][\"bounding_box\"][\"coordinates\"])\n",
    "        fr_all_place.append(tweet[\"place\"][\"name\"])\n",
    "print(\"No. FR tweets (total): \", len(fr_all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tweets:\n",
      "                     0                               1  \\\n",
      "0  1244238235351556098  Sun Mar 29 12:21:26 +0000 2020   \n",
      "1  1244238255916228608  Sun Mar 29 12:21:31 +0000 2020   \n",
      "2  1244238328943259649  Sun Mar 29 12:21:49 +0000 2020   \n",
      "3  1244238523642847234  Sun Mar 29 12:22:35 +0000 2020   \n",
      "4  1244238528025964544  Sun Mar 29 12:22:36 +0000 2020   \n",
      "\n",
      "                                                   2                3  \\\n",
      "0  [[[6.515414, 45.877464], [6.515414, 45.987096]...       Sallanches   \n",
      "1  [[[6.277249, 47.326532], [6.277249, 47.371871]...  Baume-les-Dames   \n",
      "2  [[[-5.141593, 41.364659], [-5.141593, 51.08877...           France   \n",
      "3  [[[-5.141593, 41.364659], [-5.141593, 51.08877...           France   \n",
      "4  [[[7.294318, 46.91904], [7.294318, 46.990106],...            Berne   \n",
      "\n",
      "                                                   4  \n",
      "0                           @clemd95 Serieux ?! ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±  \n",
      "1  Mirabelle incoming! ðŸ˜‹ @ Baume-les-Dames https:...  \n",
      "2                                @AkaDiane09 Fume le  \n",
      "3  ðŸ‡ªðŸ‡º L'Europe des solidaritÃ©s.ðŸ‡ªðŸ‡º\\n\\nMerci aux Al...  \n",
      "4  @charlotteepenex @AlexGubser @BaselStadt @Etat...  \n",
      "\n",
      "Covid tweets:\n",
      "                     0                               1  \\\n",
      "0  1244239823323398144  Sun Mar 29 12:27:45 +0000 2020   \n",
      "1  1244242738360524801  Sun Mar 29 12:39:20 +0000 2020   \n",
      "2  1244243050395840515  Sun Mar 29 12:40:34 +0000 2020   \n",
      "3  1244243735640256512  Sun Mar 29 12:43:18 +0000 2020   \n",
      "4  1244246277468172289  Sun Mar 29 12:53:24 +0000 2020   \n",
      "\n",
      "                                                   2         3  \\\n",
      "0  [[[7.294318, 46.91904], [7.294318, 46.990106],...     Berne   \n",
      "1  [[[-5.141593, 41.364659], [-5.141593, 51.08877...    France   \n",
      "2  [[[-5.141593, 41.364659], [-5.141593, 51.08877...    France   \n",
      "3  [[[-5.141593, 41.364659], [-5.141593, 51.08877...    France   \n",
      "4  [[[5.940459, 47.200724], [5.940459, 47.320112]...  BesanÃ§on   \n",
      "\n",
      "                                                   4  \n",
      "0  @MrsSwannH @BaselStadt @Etat_Neuchatel @Canton...  \n",
      "1  Dessin @LatuffCartoons Avec ce texte \"Imaginez...  \n",
      "2  Dessin @LatuffCartoons Avec ce texte \"Imaginez...  \n",
      "3  @Antoine_Lvq Bonjour et bravo ! Nous avons lan...  \n",
      "4                            Goupil en confinement !  \n"
     ]
    }
   ],
   "source": [
    "fr_all_tweets = pd.concat([pd.Series(fr_all_tweet_id), pd.Series(fr_all_date), pd.Series(fr_all_coord), pd.Series(fr_all_place), pd.Series(fr_all_text)], axis = 1)\n",
    "fr_covid_tweets = pd.concat([pd.Series(fr_tweet_id), pd.Series(fr_date), pd.Series(fr_coord), pd.Series(fr_place), pd.Series(fr_covid)], axis = 1)\n",
    "    \n",
    "print(\"All tweets:\")\n",
    "print(fr_all_tweets.head())\n",
    "print(\"\\nCovid tweets:\")\n",
    "print(fr_covid_tweets.head())\n",
    "\n",
    "fr_all_tweets.to_csv(\"../dataKA/FR_tweets_all.csv\", encoding = \"utf-8\")\n",
    "fr_covid_tweets.to_csv(\"../dataKA/FR_tweets_covid.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Export text of covid dataset and full dataset (FR) tor tool training\n",
    "out_fr_all_text = pd.DataFrame(fr_all_text)\n",
    "out_fr_covid = pd.DataFrame(fr_covid)\n",
    "# out_fr_all_text.to_csv(\"../dataKA/all_FR.csv\", encoding = \"utf-8\")\n",
    "# out_fr_covid.to_csv(\"../dataKA/covid_FR.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#### Define pre-processing subfunctions ####\n",
    "\n",
    "def removeNonAlphaNumChars(tweets):\n",
    "    tweets = [re.sub(r'\\-', r' ', tweet) for tweet in tweets]  # replace - with SPACE\n",
    "    tweets = [re.sub(r'[^\\w\\s]', r'', tweet) for tweet in tweets]  #remove other NON alpha numeric, excluding whitespace\n",
    "    return tweets\n",
    "\n",
    "def replaceContractions(tweets):\n",
    "    tweets = [re.sub(r\"[â€™\\']\", r' ', tweet) for tweet in tweets]  ## !!!! WEIRD APOSTROPHE BEWARE !!!\n",
    "    return tweets\n",
    "\n",
    "# FROM DEFFRO: https://github.com/Deffro/text-preprocessing-techniques\n",
    "def removeUnicode(tweets):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    tweets = [re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'[^\\x00-\\x7f]',r'',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceURL(tweets):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    tweets = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceAtUser(tweets):\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    tweets = [re.sub('@[^\\s]+','atUser',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def removeHashtagInFrontOfWord(tweets):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "# end DEFFRO\n",
    "\n",
    "\n",
    "#### DEFINE MAIN PREPROCESSING FUNCTIONS ####\n",
    "\n",
    "def cleanTweets(tweets):\n",
    "    tweets = [tweet.lower() for tweet in tweets]  #convert to lowercase\n",
    "    tweets = replaceURL(tweets) #replace URLs\n",
    "    tweets = replaceAtUser(tweets)  # replace @user\n",
    "    tweets = removeHashtagInFrontOfWord(tweets)  # remove hashtag\n",
    "    tweets = replaceContractions(tweets) # replace ' contractions with space (e.g. l'ordre = l ordre)\n",
    "    tweets = removeNonAlphaNumChars(tweets)  # remove non alphanumeric characters\n",
    "    return tweets\n",
    "\n",
    "def tokenizeTweets(tweets):\n",
    "    tokens = [nltk.word_tokenize(tweet) for tweet in tweets]\n",
    "    return tokens\n",
    "\n",
    "def applyStoplist(tokens):\n",
    "    tokens = [[word for word in token if len(word) > 2 and (word not in stoplist)] for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stemTokens(tokens):\n",
    "    tokens = [[stemmer.stem(word) for word in token] for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral:  54358\n",
      "No. positive:  6201\n",
      "No. negative:  1962\n"
     ]
    }
   ],
   "source": [
    "# Import preclassified FR tweets (football)\n",
    "fr_preClass = pd.read_csv(\"C:/Users/pang/Desktop/Stuff/_PhD/PhD Courses/_CSM/Project/preClassified/FR_3cat_worldcup.csv\")\n",
    "# fr_preClass = shuffle(fr_preClass)\n",
    "\n",
    "neutral = fr_preClass[fr_preClass[\"sentiment\"]==\"NEUTRAL\"]\n",
    "positive = fr_preClass[fr_preClass[\"sentiment\"]==\"POSITIVE\"]\n",
    "negative = fr_preClass[fr_preClass[\"sentiment\"]==\"NEGATIVE\"]\n",
    "print(\"No. neutral: \", len(neutral))\n",
    "print(\"No. positive: \", len(positive))\n",
    "print(\"No. negative: \", len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start tweet preprocessing ##\n",
    "stoplist = stopwords.words('french')\n",
    "other_stopwords = \"atUser URL\"\n",
    "stoplist = stoplist + other_stopwords.split() # final list of stopwords\n",
    "# lemmatizer = WordNetLemmatizer() # only for EN\n",
    "stemmer = SnowballStemmer('french') # set stemmer\n",
    "\n",
    "fr_preClass_pos_clean = cleanTweets(positive[\"tweet_text\"])\n",
    "fr_preClass_pos_tokens = tokenizeTweets(fr_preClass_pos_clean)\n",
    "fr_preClass_pos_tokens = applyStoplist(fr_preClass_pos_tokens)\n",
    "fr_preClass_pos_tokens = stemTokens(fr_preClass_pos_tokens)\n",
    "\n",
    "fr_preClass_neg_clean = cleanTweets(negative[\"tweet_text\"])\n",
    "fr_preClass_neg_tokens = tokenizeTweets(fr_preClass_neg_clean)\n",
    "fr_preClass_neg_tokens = applyStoplist(fr_preClass_neg_tokens)\n",
    "fr_preClass_neg_tokens = stemTokens(fr_preClass_neg_tokens)\n",
    "\n",
    "fr_preClass_neu_clean = cleanTweets(neutral[\"tweet_text\"])\n",
    "fr_preClass_neu_tokens = tokenizeTweets(fr_preClass_neu_clean)\n",
    "fr_preClass_neu_tokens = applyStoplist(fr_preClass_neu_tokens)\n",
    "fr_preClass_neu_tokens = stemTokens(fr_preClass_neu_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0        1\n",
      "28540  [infomercato, nouveau, joueur, list, agit, san...  Neutral\n",
      "32594          [oui, daccord, jou, jean, degun, remplac]  Neutral\n",
      "44218  [quier, apoyar, aquÃ­, pued, votar, voul, soute...  Neutral\n",
      "40072         [allez, tous, stad, cred, agricol, licorn]  Neutral\n",
      "37720  [gros, gros, match, shaw, auss, puain, met, yo...  Neutral\n"
     ]
    }
   ],
   "source": [
    "fr_pos_labeled = [[token, \"Positive\"] for token in fr_preClass_pos_tokens]\n",
    "fr_neg_labeled = [[token, \"Negative\"] for token in fr_preClass_neg_tokens]\n",
    "fr_neu_labeled = [[token, \"Neutral\"] for token in fr_preClass_neu_tokens]\n",
    "\n",
    "fr_dataset = fr_pos_labeled + fr_neg_labeled + fr_neu_labeled\n",
    "fr_dataset = shuffle(pd.DataFrame(fr_dataset))\n",
    "fr_dataset_train = fr_dataset[:43766]\n",
    "fr_dataset_test = fr_dataset[43766:]\n",
    "print(fr_dataset_train[:5])\n",
    "\n",
    "# split features and labels for SKLEARN classifier\n",
    "features = fr_dataset.loc[:, 0]\n",
    "labels  = fr_dataset.loc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. tweets in dictionary:  62521\n",
      "[({'dÃ©but': True, 'championnat': True, 'anglais': True, 'voit': True, 'vrai': True, 'Ã©volu': True, 'plusieur': True, 'domain': True, 'com': True, 'back': True, 'red': True}, 'Neutral'), ({'08h00': True, 'rediffu': True, 'chels': True, 'inter': True, 'milan': True, 'international': True, 'champion': True, 'cup': True, 'bein1': True}, 'Neutral'), ({'traduis': True, 'tel': True, 'envi': True, 'cass': True, 'club': True, 'tanch': True, 'pouv': True, 'plus': True}, 'Neutral')]\n"
     ]
    }
   ],
   "source": [
    "## Convert pos/neg/neu tokens to a dictionary for NLTK NB Classifier ##\n",
    "\n",
    "def createDict(tokenList):\n",
    "    for tweetTokens in tokenList:\n",
    "        yield dict([token, True] for token in tweetTokens)\n",
    "\n",
    "posDict = createDict(fr_preClass_pos_tokens)\n",
    "negDict = createDict(fr_preClass_neg_tokens)\n",
    "neuDict = createDict(fr_preClass_neu_tokens)\n",
    "\n",
    "positiveDataset = [(tweet_dict, \"Positive\") for tweet_dict in posDict]\n",
    "negativeDataset = [(tweet_dict, \"Negative\") for tweet_dict in negDict]\n",
    "neutralDataset = [(tweet_dict, \"Neutral\") for tweet_dict in neuDict]\n",
    "modelData = positiveDataset + negativeDataset + neutralDataset\n",
    "modelData = shuffle(modelData)\n",
    "\n",
    "print(\"No. tweets in dictionary: \", len(modelData))\n",
    "print(modelData[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy (NLTK NB classifier): 0.6305518528392429\n"
     ]
    }
   ],
   "source": [
    "## NAIVE BAYES FROM NLTK ##\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "# Training/test set\n",
    "fr_train = modelData[:43766] #70%\n",
    "fr_test = modelData[43766:] #30%\n",
    "\n",
    "# NB classifier\n",
    "NB_classifier = NaiveBayesClassifier.train(fr_train)\n",
    "\n",
    "print(\"\\nAccuracy (NLTK NB classifier):\", classify.accuracy(NB_classifier, fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for classifier training (SKLEARN) ##\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=dummy,preprocessor=dummy,max_df=0.9, min_df=10)\n",
    "X=cv.fit_transform(features).toarray()\n",
    "y=labels\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAIVE BAYES USING SKLEARN ##\n",
    "\n",
    "#Import Multinomial Naive Bayes model (https://towardsdatascience.com/sentiment-analysis-of-tweets-using-multinomial-naive-bayes-1009ed24276b)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a multinomial NB Classifier\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_NB = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST USING SKLEARN ##\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFclassifier=RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "RFclassifier.fit(X_train,y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred_RF = RFclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.19      0.52      0.28       590\n",
      "     Neutral       0.95      0.75      0.84     16287\n",
      "    Positive       0.30      0.67      0.41      1880\n",
      "\n",
      "    accuracy                           0.73     18757\n",
      "   macro avg       0.48      0.65      0.51     18757\n",
      "weighted avg       0.86      0.73      0.78     18757\n",
      "\n",
      "\n",
      "Accuracy (NB): 0.7346590606173695\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.35      0.04      0.07       590\n",
      "     Neutral       0.88      0.98      0.93     16287\n",
      "    Positive       0.49      0.15      0.23      1880\n",
      "\n",
      "    accuracy                           0.87     18757\n",
      "   macro avg       0.58      0.39      0.41     18757\n",
      "weighted avg       0.83      0.87      0.83     18757\n",
      "\n",
      "\n",
      "Accuracy (RF): 0.8673028735938583\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y_test, y_pred_NB))\n",
    "print(\"\\nAccuracy (NB):\", metrics.accuracy_score(y_test, y_pred_NB))\n",
    "\n",
    "print (\"\\n\", classification_report(y_test, y_pred_RF))\n",
    "print(\"\\nAccuracy (RF):\", metrics.accuracy_score(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  type label                                               text\n",
      "0  all   neu                           @clemd95 Serieux ?! ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±\n",
      "1  all   pos  Mirabelle incoming! ðŸ˜‹ @ Baume-les-Dames https:...\n",
      "2  all   neu                                @AkaDiane09 Fume le\n",
      "3  all   pos  ðŸ‡ªðŸ‡º L'Europe des solidaritÃ©s.ðŸ‡ªðŸ‡ºMerci aux Allema...\n",
      "4  all   neu  @charlotteepenex @AlexGubser @BaselStadt @Etat...\n",
      "500\n",
      "    type label                                               text\n",
      "0  covid   neg  @MrsSwannH @BaselStadt @Etat_Neuchatel @Canton...\n",
      "1  covid   neu  Dessin @LatuffCartoons Avec ce texte \"Imaginez...\n",
      "2  covid   neu  Dessin @LatuffCartoons Avec ce texte \"Imaginez...\n",
      "3  covid   pos  @Antoine_Lvq Bonjour et bravo ! Nous avons lan...\n",
      "4  covid   neu                            Goupil en confinement !\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "## load manually classified tweets (CH/Covid)\n",
    "\n",
    "fr_labeled_all = pd.read_csv(\"../dataKA/frlabeledAll1.csv\")\n",
    "fr_labeled_cov = pd.read_csv(\"../dataKA/frlabeledCovid1.csv\")\n",
    "print(fr_labeled_all.head())\n",
    "print(len(fr_labeled_all))\n",
    "print(fr_labeled_cov.head())\n",
    "print(len(fr_labeled_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral (all):  371\n",
      "No. positive (all):  75\n",
      "No. negative (all):  54\n"
     ]
    }
   ],
   "source": [
    "# ALL\n",
    "neu = fr_labeled_all[fr_labeled_all[\"label\"]==\"neu\"]\n",
    "pos = fr_labeled_all[fr_labeled_all[\"label\"]==\"pos\"]\n",
    "neg = fr_labeled_all[fr_labeled_all[\"label\"]==\"neg\"]\n",
    "print(\"No. neutral (all): \", len(neu))\n",
    "print(\"No. positive (all): \", len(pos))\n",
    "print(\"No. negative (all): \", len(neg))\n",
    "\n",
    "## Start preprocessing ##\n",
    "fr_labeled_all_pos_clean = cleanTweets(pos[\"text\"])\n",
    "fr_labeled_all_pos_tokens = tokenizeTweets(fr_labeled_all_pos_clean)\n",
    "fr_labeled_all_pos_tokens = applyStoplist(fr_labeled_all_pos_tokens)\n",
    "fr_labeled_all_pos_tokens = stemTokens(fr_labeled_all_pos_tokens)\n",
    "\n",
    "fr_labeled_all_neg_clean = cleanTweets(neg[\"text\"])\n",
    "fr_labeled_all_neg_tokens = tokenizeTweets(fr_labeled_all_neg_clean)\n",
    "fr_labeled_all_neg_tokens = applyStoplist(fr_labeled_all_neg_tokens)\n",
    "fr_labeled_all_neg_tokens = stemTokens(fr_labeled_all_neg_tokens)\n",
    "\n",
    "fr_labeled_all_neu_clean = cleanTweets(neu[\"text\"])\n",
    "fr_labeled_all_neu_tokens = tokenizeTweets(fr_labeled_all_neu_clean)\n",
    "fr_labeled_all_neu_tokens = applyStoplist(fr_labeled_all_neu_tokens)\n",
    "fr_labeled_all_neu_tokens = stemTokens(fr_labeled_all_neu_tokens)\n",
    "\n",
    "\n",
    "fr_pos_labeled_all = [[token, \"Positive\"] for token in fr_labeled_all_pos_tokens]\n",
    "fr_neg_labeled_all = [[token, \"Negative\"] for token in fr_labeled_all_neg_tokens]\n",
    "fr_neu_labeled_all = [[token, \"Neutral\"] for token in fr_labeled_all_neu_tokens]\n",
    "\n",
    "fr_dataset_all = fr_pos_labeled_all + fr_neg_labeled_all + fr_neu_labeled_all\n",
    "fr_dataset_all = shuffle(pd.DataFrame(fr_dataset_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral (covid):  414\n",
      "No. positive (covid):  44\n",
      "No. negative (covid):  42\n"
     ]
    }
   ],
   "source": [
    "# COVID\n",
    "neu_cov = fr_labeled_cov[fr_labeled_cov[\"label\"]==\"neu\"]\n",
    "pos_cov = fr_labeled_cov[fr_labeled_cov[\"label\"]==\"pos\"]\n",
    "neg_cov = fr_labeled_cov[fr_labeled_cov[\"label\"]==\"neg\"]\n",
    "print(\"No. neutral (covid): \", len(neu_cov))\n",
    "print(\"No. positive (covid): \", len(pos_cov))\n",
    "print(\"No. negative (covid): \", len(neg_cov))\n",
    "\n",
    "## Start preprocessing ##\n",
    "fr_labeled_cov_pos_clean = cleanTweets(pos_cov[\"text\"])\n",
    "fr_labeled_cov_pos_tokens = tokenizeTweets(fr_labeled_cov_pos_clean)\n",
    "fr_labeled_cov_pos_tokens = applyStoplist(fr_labeled_cov_pos_tokens)\n",
    "fr_labeled_cov_pos_tokens = stemTokens(fr_labeled_cov_pos_tokens)\n",
    "\n",
    "fr_labeled_cov_neg_clean = cleanTweets(neg_cov[\"text\"])\n",
    "fr_labeled_cov_neg_tokens = tokenizeTweets(fr_labeled_cov_neg_clean)\n",
    "fr_labeled_cov_neg_tokens = applyStoplist(fr_labeled_cov_neg_tokens)\n",
    "fr_labeled_cov_neg_tokens = stemTokens(fr_labeled_cov_neg_tokens)\n",
    "\n",
    "fr_labeled_cov_neu_clean = cleanTweets(neu_cov[\"text\"])\n",
    "fr_labeled_cov_neu_tokens = tokenizeTweets(fr_labeled_cov_neu_clean)\n",
    "fr_labeled_cov_neu_tokens = applyStoplist(fr_labeled_cov_neu_tokens)\n",
    "fr_labeled_cov_neu_tokens = stemTokens(fr_labeled_cov_neu_tokens)\n",
    "\n",
    "fr_pos_labeled_cov = [[token, \"Positive\"] for token in fr_labeled_cov_pos_tokens]\n",
    "fr_neg_labeled_cov = [[token, \"Negative\"] for token in fr_labeled_cov_neg_tokens]\n",
    "fr_neu_labeled_cov = [[token, \"Neutral\"] for token in fr_labeled_cov_neu_tokens]\n",
    "\n",
    "fr_dataset_cov = fr_pos_labeled_cov + fr_neg_labeled_cov + fr_neu_labeled_cov\n",
    "fr_dataset_cov = shuffle(pd.DataFrame(fr_dataset_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. features (preclassified):  62521\n",
      "No. features (all):  500\n",
      "No. features (covid):  500\n"
     ]
    }
   ],
   "source": [
    "# split features and labels for SKLEARN classifier\n",
    "features_all = fr_dataset_all.loc[:, 0]\n",
    "labels_all  = fr_dataset_all.loc[:, 1]\n",
    "features_cov = fr_dataset_cov.loc[:, 0]\n",
    "labels_cov  = fr_dataset_cov.loc[:, 1]\n",
    "\n",
    "features1 = pd.concat([features_all, features], axis=0)\n",
    "labels1  = pd.concat([labels_all, labels], axis=0)\n",
    "\n",
    "features2 = pd.concat([features_cov, features], axis=0)\n",
    "labels2  = pd.concat([labels_cov, labels], axis=0)\n",
    "\n",
    "print(\"No. features (preclassified): \", len(features))\n",
    "print(\"No. features (all): \", len(features1)-len(features))\n",
    "print(\"No. features (covid): \", len(features2)-len(features))\n",
    "\n",
    "# print(features[:3])\n",
    "# print(features1[:3])\n",
    "# print(features1[249:252])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  500\n",
      "359    Neutral\n",
      "349    Neutral\n",
      "189    Neutral\n",
      "Name: 1, dtype: object\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.30      0.41      0.35        54\n",
      "     Neutral       0.81      0.71      0.75       371\n",
      "    Positive       0.31      0.43      0.36        75\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.47      0.51      0.49       500\n",
      "weighted avg       0.68      0.63      0.65       500\n",
      "\n",
      "\n",
      "Accuracy (NB) of manually classified tweets, all: 0.632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      0.02      0.04        54\n",
      "     Neutral       0.75      0.98      0.85       371\n",
      "    Positive       0.47      0.09      0.16        75\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.57      0.36      0.35       500\n",
      "weighted avg       0.68      0.74      0.66       500\n",
      "\n",
      "\n",
      "Accuracy (RF) of manually classified tweets, all: 0.742\n"
     ]
    }
   ],
   "source": [
    "## use manually labelled tweets (all) only as test set ##\n",
    "\n",
    "X1=cv.fit_transform(features1).toarray()\n",
    "y1=labels1\n",
    "# X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 249, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "test_all_index = 500 # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_test1, X_train1 = X1[:test_all_index], X1[test_all_index:]\n",
    "y_test1, y_train1 = y1[:test_all_index], y1[test_all_index:]\n",
    "print(\"Size (test set): \", len(X_test1))\n",
    "print(y_test1[:3])\n",
    "\n",
    "# sklearn multinomial NB classifier #\n",
    "mnb.fit(X_train1, y_train1)\n",
    "y_pred_NB1 = mnb.predict(X_test1)\n",
    "print (classification_report(y_test1, y_pred_NB1))\n",
    "print(\"\\nAccuracy (NB) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_NB1))\n",
    "\n",
    "# sklearn random forest classfier #\n",
    "RFclassifier.fit(X_train1,y_train1)\n",
    "y_pred_RF1 = RFclassifier.predict(X_test1)\n",
    "print (classification_report(y_test1, y_pred_RF1))\n",
    "print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_RF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  500\n",
      "359     Neutral\n",
      "30     Positive\n",
      "51     Negative\n",
      "Name: 1, dtype: object\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.14      0.74      0.24        42\n",
      "     Neutral       0.92      0.20      0.33       414\n",
      "    Positive       0.18      0.80      0.30        44\n",
      "\n",
      "    accuracy                           0.30       500\n",
      "   macro avg       0.42      0.58      0.29       500\n",
      "weighted avg       0.79      0.30      0.32       500\n",
      "\n",
      "\n",
      "Accuracy (NB) of manually classified tweets, all: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        42\n",
      "     Neutral       0.84      0.98      0.90       414\n",
      "    Positive       0.41      0.16      0.23        44\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.42      0.38      0.38       500\n",
      "weighted avg       0.73      0.82      0.77       500\n",
      "\n",
      "\n",
      "Accuracy (RF) of manually classified tweets, all: 0.822\n"
     ]
    }
   ],
   "source": [
    "## use manually labelled tweets (covid) only as test set ##\n",
    "\n",
    "X2=cv.fit_transform(features2).toarray()\n",
    "y2=labels2\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 250, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "test_cov_index = 500 # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_test2, X_train2 = X2[:test_cov_index], X2[test_cov_index:]\n",
    "y_test2, y_train2 = y2[:test_cov_index], y2[test_cov_index:]\n",
    "print(\"Size (test set): \", len(X_test2))\n",
    "print(y_test2[:3])\n",
    "\n",
    "# sklearn multinomial NB classifier #\n",
    "mnb.fit(X_train2, y_train2)\n",
    "y_pred_NB2 = mnb.predict(X_test2)\n",
    "print (classification_report(y_test2, y_pred_NB2))\n",
    "print(\"\\nAccuracy (NB) of manually classified tweets, all:\", metrics.accuracy_score(y_test2, y_pred_NB2))\n",
    "\n",
    "# sklearn random forest classfier #\n",
    "RFclassifier.fit(X_train2, y_train2)\n",
    "y_pred_RF2 = RFclassifier.predict(X_test2)\n",
    "print (classification_report(y_test2, y_pred_RF2))\n",
    "print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test2, y_pred_RF2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  77294\n"
     ]
    }
   ],
   "source": [
    "## Apply trained tool to predict polarity of collected FR tweets ##\n",
    "\n",
    "# fr_all_tweets (id, date, coords, place, text)\n",
    "# fr_all_tweets (id, date, coords, place, text)\n",
    "\n",
    "# Using sklearn mNB classifier\n",
    "labels_all = pd.Series([\"tbc\"] * len(fr_all_tweets))\n",
    "# features_all = fr_all_tweets.loc[:,4]\n",
    "features_all = cleanTweets(fr_all_tweets.loc[:,4])\n",
    "features_all = tokenizeTweets(features_all)\n",
    "features_all = applyStoplist(features_all)\n",
    "features_all = stemTokens(features_all)\n",
    "features_all = pd.Series(features_all)\n",
    "labels_train = labels\n",
    "features_train = features\n",
    "\n",
    "# print(len(labels_all))\n",
    "# print(len(features_all))\n",
    "# print(len(labels_train))\n",
    "# print(len(features_train))\n",
    "# print(type(labels_all))\n",
    "# print(type(features_all))\n",
    "# print(type(labels_train))\n",
    "# print(type(features_train))\n",
    "\n",
    "X_all = cv.fit_transform(pd.concat([features_train, features_all], axis=0)).toarray()\n",
    "y_all = pd.concat([labels_train, labels_train], axis=0)\n",
    "\n",
    "cv.fit_transform(features2).toarray()\n",
    "train_index = len(features_train) # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_all_train, X_all_test = X_all[:train_index], X_all[train_index:]\n",
    "y_all_train, y_all_test = y_all[:train_index], y_all[train_index:]\n",
    "\n",
    "print(\"Size (test set): \", len(X_all_test))\n",
    "# print(type(X_all_train))\n",
    "# print(type(X_all_test))\n",
    "# print(type(y_all_train))\n",
    "# print(type(y_all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_all_train, y_all_train)\n",
    "y_all_pred = mnb.predict(X_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. predictions:  77294\n",
      "['Neutral' 'Neutral' 'Neutral' 'Positive' 'Neutral' 'Neutral' 'Positive'\n",
      " 'Neutral' 'Neutral' 'Positive']\n",
      "                     0                               1  \\\n",
      "0  1244238235351556098  Sun Mar 29 12:21:26 +0000 2020   \n",
      "1  1244238255916228608  Sun Mar 29 12:21:31 +0000 2020   \n",
      "2  1244238328943259649  Sun Mar 29 12:21:49 +0000 2020   \n",
      "3  1244238523642847234  Sun Mar 29 12:22:35 +0000 2020   \n",
      "4  1244238528025964544  Sun Mar 29 12:22:36 +0000 2020   \n",
      "\n",
      "                                                   2                3  \\\n",
      "0  [[[6.515414, 45.877464], [6.515414, 45.987096]...       Sallanches   \n",
      "1  [[[6.277249, 47.326532], [6.277249, 47.371871]...  Baume-les-Dames   \n",
      "2  [[[-5.141593, 41.364659], [-5.141593, 51.08877...           France   \n",
      "3  [[[-5.141593, 41.364659], [-5.141593, 51.08877...           France   \n",
      "4  [[[7.294318, 46.91904], [7.294318, 46.990106],...            Berne   \n",
      "\n",
      "                                                   4         5  \n",
      "0                           @clemd95 Serieux ?! ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±   Neutral  \n",
      "1  Mirabelle incoming! ðŸ˜‹ @ Baume-les-Dames https:...   Neutral  \n",
      "2                                @AkaDiane09 Fume le   Neutral  \n",
      "3  ðŸ‡ªðŸ‡º L'Europe des solidaritÃ©s.ðŸ‡ªðŸ‡º\\n\\nMerci aux Al...  Positive  \n",
      "4  @charlotteepenex @AlexGubser @BaselStadt @Etat...   Neutral  \n"
     ]
    }
   ],
   "source": [
    "# res_all = pd.concat([pd.Series(y_all_pred), pd.Series(fr_all_tweets.loc[:,4])], axis = 1) \n",
    "res_all = pd.concat([pd.Series(fr_all_tweet_id), pd.Series(fr_all_date), \n",
    "                     pd.Series(fr_all_coord), pd.Series(fr_all_place), \n",
    "                     pd.Series(fr_all_text), pd.Series(y_all_pred)], \n",
    "                    axis = 1) \n",
    "res_all.to_csv(\"../dataKA/res_all_FR.csv\", encoding = \"utf-8\")\n",
    "print(\"No. predictions: \", len(y_all_pred))\n",
    "print(y_all_pred[:10])\n",
    "print(res_all[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
