{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "Dateset 1:  7410 from evalita16 dataset  http://www.di.unito.it/~tutreeb/sentipolc-evalita16/data.html       \n",
    "Dataset 2:  1989 from evalita16 dataset2 datset link (http:???) total number of tweets 4513 of which 1989 rehydrated and available    \n",
    "Dataset 3:  165815 from an open dataset for sentiment analysis: https://github.com/charlesmalafosse/open-dataset-for-sentiment-analysis   \n",
    " \n",
    "For dataset 1 and 2 see also:   \n",
    "http://www.di.unito.it/~tutreeb/sentipolc-evalita16/sentipolc-guidelines2016UPDATED130916.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training/testing datasets\n",
    "imp_directory = r'C:\\Users\\barsanti\\Desktop\\Courses\\Computational social media\\project\\2_training tools\\preclassified_data'\n",
    "\n",
    "dataset1_path = r'\\ita_training_set_sentipolc16.csv'\n",
    "data1 = pd.read_csv(imp_directory + dataset1_path)\n",
    "\n",
    "dataset2_path = r'\\ita_kar_all_tweets.pkl'\n",
    "pickle2_in = open(imp_directory + dataset2_path, 'rb')\n",
    "data2 = pickle.load(pickle2_in)\n",
    "\n",
    "dataset3_path = r'\\ita_betsentiment-IT-tweets-sentiment-players.csv'\n",
    "data3 = pd.read_csv(imp_directory + dataset3_path, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define pre-processing subfunctions ####\n",
    "\n",
    "def removeNonAlphaNumChars(tweets):\n",
    "    tweets = [re.sub(r'\\-', r' ', tweet) for tweet in tweets]  # replace - with SPACE\n",
    "    tweets = [re.sub(r'[^\\w\\s]', r'', tweet) for tweet in tweets]  #remove other NON alpha numeric, excluding whitespace\n",
    "    return tweets\n",
    "\n",
    "def replaceContractions(tweets):\n",
    "    tweets = [re.sub(r\"[‚Äô\\']\", r' ', tweet) for tweet in tweets]  ## !!!! WEIRD APOSTROPHE BEWARE !!!\n",
    "    return tweets\n",
    "\n",
    "# FROM DEFFRO: https://github.com/Deffro/text-preprocessing-techniques\n",
    "def removeUnicode(tweets):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    tweets = [re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'[^\\x00-\\x7f]',r'',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceURL(tweets):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    tweets = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceAtUser(tweets):\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    tweets = [re.sub('@[^\\s]+','atUser',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def removeHashtagInFrontOfWord(tweets):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "# end DEFFRO\n",
    "\n",
    "\n",
    "#### DEFINE MAIN PREPROCESSING FUNCTIONS ####\n",
    "\n",
    "def cleanTweets(tweets):\n",
    "    tweets = [tweet.lower() for tweet in tweets]  #convert to lowercase\n",
    "    tweets = replaceURL(tweets) #replace URLs\n",
    "    tweets = replaceAtUser(tweets)  # replace @user\n",
    "    tweets = removeHashtagInFrontOfWord(tweets)  # remove hashtag\n",
    "    tweets = replaceContractions(tweets) # replace ' contractions with space (e.g. l'ordre = l ordre)\n",
    "    tweets = removeNonAlphaNumChars(tweets)  # remove non alphanumeric characters\n",
    "    return tweets\n",
    "\n",
    "def tokenizeTweets(tweets):\n",
    "    tokens = [nltk.word_tokenize(tweet) for tweet in tweets]\n",
    "    return tokens\n",
    "\n",
    "def applyStoplist(tokens):\n",
    "    tokens = [[word for word in token if len(word) > 2 and (word not in stoplist)] for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stemTokens(tokens):\n",
    "    tokens = [[stemmer.stem(word) for word in token] for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseremoval_and_labelling(tweets, polarity):\n",
    "\n",
    "    tweets_clean = cleanTweets(tweets)\n",
    "    tweets_tokens = tokenizeTweets(tweets_clean)\n",
    "    tweets_tokens = applyStoplist(tweets_tokens)\n",
    "    tweets_tokens = stemTokens(tweets_tokens)\n",
    "    \n",
    "    tweets_labeled = [[token, polarity] for token in tweets_tokens]\n",
    "    \n",
    "    return tweets_labeled, tweets_tokens\n",
    "\n",
    "def noiseremoval(tweets):\n",
    "\n",
    "    tweets_clean = cleanTweets(tweets)\n",
    "    tweets_tokens = tokenizeTweets(tweets_clean)\n",
    "    tweets_tokens = applyStoplist(tweets_tokens)\n",
    "    tweets_tokens = stemTokens(tweets_tokens)\n",
    "    \n",
    "    return tweets_tokens\n",
    "\n",
    "## Convert pos/neg/neu tokens to a dictionary for NLTK NB Classifier ##\n",
    "def createDict(tokenList):\n",
    "    for tweetTokens in tokenList:\n",
    "        yield dict([token, True] for token in tweetTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preclassified tweets import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reorganize all datasets into a unique dataframe\n",
    "all_tweets = pd.DataFrame(0, index = [0], columns = ['origin','id','text','pol'])\n",
    "\n",
    "for index in data1.index:\n",
    "    polarity = data1.loc[index,'opos'] - data1.loc[index,'oneg']\n",
    "    all_tweets.loc[index,:] = ['evalita16', data1.loc[index,'idtwitter'], data1.loc[index,'text'], polarity]\n",
    "    \n",
    "for index in data2.index:\n",
    "    polarity = data2.loc[index,'pos'] - data2.loc[index,'neg']\n",
    "    all_tweets.loc[all_tweets.shape[0]+1,:] = ['karolos', data2.loc[index,'dtwitter'], data2.loc[index,'TEXT'], polarity]\n",
    "\n",
    "for index in range(50000):\n",
    "    if data3.loc[index,'sentiment'] == 'NEUTRAL':\n",
    "        polarity = 0\n",
    "    elif data3.loc[index,'sentiment'] == 'POSITIVE':\n",
    "        polarity = 1\n",
    "    elif data3.loc[index,'sentiment'] == 'NEGATIVE':\n",
    "        polarity = -1\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    all_tweets.loc[all_tweets.shape[0]+1,:] = ['sport', data3.loc[index,'tweet_id'], data3.loc[index,'tweet_text'], polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral:  44932\n",
      "No. positive:  9048\n",
      "No. negative:  5260\n"
     ]
    }
   ],
   "source": [
    "neutral = all_tweets.loc[all_tweets.loc[:,\"pol\"]== 0]\n",
    "positive = all_tweets.loc[all_tweets.loc[:,\"pol\"]== 1]\n",
    "negative = all_tweets.loc[all_tweets.loc[:,\"pol\"]== -1]\n",
    "print(\"No. neutral: \", len(neutral))\n",
    "print(\"No. positive: \", len(positive))\n",
    "print(\"No. negative: \", len(negative))\n",
    "total = len(neutral) + len(positive) + len(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0         1\n",
      "51184                                          [bentorn]   NEUTRAL\n",
      "39715                                   [apprec, pogba6]   NEUTRAL\n",
      "27538  [55mln, ricav, magliett, sol, 24h, ronaldoalla...   NEUTRAL\n",
      "23438  [oggi, poi, quand, chied, qual, giorn, bell, v...   NEUTRAL\n",
      "11683  [govern, mont, mirabil, esemp, sinerg, fra, we...  NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "lang = 'italian'\n",
    "# Clean, tokenize and label the tweets\n",
    "stoplist = stopwords.words(lang)\n",
    "other_stopwords = \"atUser URL\"\n",
    "stoplist = stoplist + other_stopwords.split() # final list of stopwords\n",
    "stemmer = SnowballStemmer(lang) # set stemmer\n",
    "    \n",
    "it_neu_labeled, it_preClass_neu_tokens = noiseremoval_and_labelling(neutral.loc[:,'text'], 'NEUTRAL')\n",
    "it_pos_labeled, it_preClass_pos_tokens = noiseremoval_and_labelling(positive.loc[:,'text'], 'POSITIVE')\n",
    "it_neg_labeled, it_preClass_neg_tokens = noiseremoval_and_labelling(negative.loc[:,'text'], 'NEGATIVE')\n",
    "\n",
    "it_dataset = it_pos_labeled + it_neg_labeled + it_neu_labeled\n",
    "it_dataset = shuffle(pd.DataFrame(it_dataset))\n",
    "it_dataset_train = it_dataset[:int(total*0.7)]\n",
    "it_dataset_test = it_dataset[int(total*0.7):]\n",
    "print(it_dataset_train[:5])\n",
    "\n",
    "# split features and labels for SKLEARN classifier\n",
    "features = it_dataset.loc[:, 0]\n",
    "labels  = it_dataset.loc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK NB Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert pos/neg/neu tokens to a dictionary for NLTK NB Classifier ##\n",
    "posDict = createDict(it_preClass_pos_tokens)\n",
    "negDict = createDict(it_preClass_neg_tokens)\n",
    "neuDict = createDict(it_preClass_neu_tokens)\n",
    "\n",
    "positiveDataset = [(tweet_dict, \"Positive\") for tweet_dict in posDict]\n",
    "negativeDataset = [(tweet_dict, \"Negative\") for tweet_dict in negDict]\n",
    "neutralDataset = [(tweet_dict, \"Neutral\") for tweet_dict in neuDict]\n",
    "modelData = positiveDataset + negativeDataset + neutralDataset\n",
    "modelData = shuffle(modelData)\n",
    "\n",
    "print(\"No. tweets in dictionary: \", len(modelData))\n",
    "print(modelData[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAIVE BAYES FROM NLTK ##\n",
    "# NB classifier\n",
    "\n",
    "# Training/test set\n",
    "it_train = modelData[:int(total*0.7)] #70%\n",
    "it_test = modelData[int(total*0.7):] #30%\n",
    "\n",
    "NB_classifier = NaiveBayesClassifier.train(it_train)\n",
    "\n",
    "print(\"\\nAccuracy (NLTK NB classifier):\", classify.accuracy(NB_classifier, it_test ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SKLEARN  classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for classifier training (SKLEARN) ##\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=dummy,preprocessor=dummy,max_df=0.9, min_df=10)\n",
    "X=cv.fit_transform(features).toarray()\n",
    "y=labels\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAIVE BAYES USING SKLEARN ##\n",
    "\n",
    "#Import Multinomial Naive Bayes model (https://towardsdatascience.com/sentiment-analysis-of-tweets-using-multinomial-naive-bayes-1009ed24276b)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a multinomial NB Classifier\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_NB = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST USING SKLEARN ##\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFclassifier=RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "RFclassifier.fit(X_train,y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred_RF = RFclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.44      0.51      0.47      1571\n",
      "     NEUTRAL       0.86      0.87      0.86     13520\n",
      "    POSITIVE       0.55      0.46      0.50      2681\n",
      "\n",
      "    accuracy                           0.78     17772\n",
      "   macro avg       0.62      0.61      0.61     17772\n",
      "weighted avg       0.77      0.78      0.77     17772\n",
      "\n",
      "\n",
      "Accuracy (NB): 0.7757146072473554\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "# NB report\n",
    "print (classification_report(y_test, y_pred_NB))\n",
    "print(\"\\nAccuracy (NB):\", metrics.accuracy_score(y_test, y_pred_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest report\n",
    "print (\"\\n\", classification_report(y_test, y_pred_RF))\n",
    "print(\"\\nAccuracy (RF):\", metrics.accuracy_score(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with manually classified tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\barsanti\\Desktop\\Courses\\Computational social media\\project\\2_training tools\"\n",
    "ita_doc_all = r\"\\tweets_manual_class_it.xlsx\"\n",
    "ita_doc_cov = r\"\\tweets_manual_class_it_covid.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                   id         day  hour  \\\n",
      "0           0  1244239906630775040  2020-03-29    12   \n",
      "1           1  1244240206485742080  2020-03-29    12   \n",
      "2           2  1244240370415930880  2020-03-29    12   \n",
      "3           3  1244240702164340992  2020-03-29    12   \n",
      "4           4  1244241096156332032  2020-03-29    12   \n",
      "\n",
      "                             time  \\\n",
      "0  Sun Mar 29 12:28:05 +0000 2020   \n",
      "1  Sun Mar 29 12:29:16 +0000 2020   \n",
      "2  Sun Mar 29 12:29:55 +0000 2020   \n",
      "3  Sun Mar 29 12:31:14 +0000 2020   \n",
      "4  Sun Mar 29 12:32:48 +0000 2020   \n",
      "\n",
      "                                                text lang  \\\n",
      "0                             @LordOfVenice Forza...   it   \n",
      "1                    @Crocket81893187 :-( \\nForza...   it   \n",
      "2  Tra i pochi campionati ancora attivi, quello d...   it   \n",
      "3  Desideriamo offire il nostro contributo concre...   it   \n",
      "4  Aspettando che tutto torni come prima anche se...   it   \n",
      "\n",
      "                                               place pol covid  \n",
      "0  {'id': '80c1046e4fdaa8c9', 'url': 'https://api...   1    no  \n",
      "1  {'id': '80c1046e4fdaa8c9', 'url': 'https://api...   1    no  \n",
      "2  {'id': 'ee3f5e5627afb978', 'url': 'https://api...   0    no  \n",
      "3  {'id': '69a6f7855b30766d', 'url': 'https://api...   0    no  \n",
      "4  {'id': '61c9f8f0c1d1ec61', 'url': 'https://api...   0   yes  \n",
      "471\n",
      "   Unnamed: 0                   id         day  hour  \\\n",
      "0           0  1244241096156332032  2020-03-29    12   \n",
      "1           1  1244241533852933888  2020-03-29    12   \n",
      "2           2  1244242137417466112  2020-03-29    12   \n",
      "3           3  1244246506695246080  2020-03-29    12   \n",
      "4           4  1244253447186244096  2020-03-29    13   \n",
      "\n",
      "                             time  \\\n",
      "0  Sun Mar 29 12:32:48 +0000 2020   \n",
      "1  Sun Mar 29 12:34:33 +0000 2020   \n",
      "2  Sun Mar 29 12:36:57 +0000 2020   \n",
      "3  Sun Mar 29 12:54:18 +0000 2020   \n",
      "4  Sun Mar 29 13:21:53 +0000 2020   \n",
      "\n",
      "                                                text lang  \\\n",
      "0  Aspettando che tutto torni come prima anche se...   it   \n",
      "1  üòç Negozio online Infiorescenza di cannabis lig...   it   \n",
      "2  Olio di CBD 8-15-25-30%: ‚ù§Ô∏èüîù https://t.co/5VTb...   it   \n",
      "3  Fxxk the virus part 16.\\n#covid19 #killitbyfoo...   it   \n",
      "4  Eccoli qui, puntuali, che non vedevano l'ora d...   it   \n",
      "\n",
      "                                               place pol covid  \n",
      "0  {'id': '61c9f8f0c1d1ec61', 'url': 'https://api...   0   yes  \n",
      "1  {'id': 'c799e2d3a79f810e', 'url': 'https://api...   0   yes  \n",
      "2  {'id': 'c799e2d3a79f810e', 'url': 'https://api...   0   yes  \n",
      "3  {'id': 'cd661902b07eb657', 'url': 'https://api...  -1   yes  \n",
      "4  {'id': '22eb26a98cbcea30', 'url': 'https://api...  -1   yes  \n",
      "344\n"
     ]
    }
   ],
   "source": [
    "## load manually classified tweets (CH/Covid)\n",
    "it_labeled_all = pd.read_excel(folder_path + ita_doc_all)\n",
    "it_labeled_cov = pd.read_excel(folder_path + ita_doc_cov)\n",
    "print(it_labeled_all.head())\n",
    "print(len(it_labeled_all))\n",
    "print(it_labeled_cov.head())\n",
    "print(len(it_labeled_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral (all):  90\n",
      "No. positive (all):  60\n",
      "No. negative (all):  61\n"
     ]
    }
   ],
   "source": [
    "# All tweets\n",
    "neu = it_labeled_all.loc[it_labeled_all.loc[:,\"pol\"]== 0]\n",
    "pos = it_labeled_all.loc[it_labeled_all.loc[:,\"pol\"]== 1]\n",
    "neg = it_labeled_all.loc[it_labeled_all.loc[:,\"pol\"]== -1]\n",
    "print(\"No. neutral (all): \", len(neu))\n",
    "print(\"No. positive (all): \", len(pos))\n",
    "print(\"No. negative (all): \", len(neg))\n",
    "tot = len(neu) + len(pos) + len(neg)\n",
    "\n",
    "it_neu_all_labeled, it_preClass_neu_all_tokens = noiseremoval_and_labelling(neu.loc[:,'text'], 'NEUTRAL')\n",
    "it_pos_all_labeled, it_preClass_pos_all_tokens = noiseremoval_and_labelling(pos.loc[:,'text'], 'POSITIVE')\n",
    "it_neg_all_labeled, it_preClass_neg_all_tokens = noiseremoval_and_labelling(neg.loc[:,'text'], 'NEGATIVE')\n",
    "\n",
    "it_dataset_all = it_pos_all_labeled + it_neg_all_labeled + it_neu_all_labeled\n",
    "it_dataset_all = shuffle(pd.DataFrame(it_dataset_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral (cov):  90\n",
      "No. positive (cov):  60\n",
      "No. negative (cov):  61\n"
     ]
    }
   ],
   "source": [
    "# Covid tweets\n",
    "neu_cov = it_labeled_cov.loc[it_labeled_cov.loc[:,\"pol\"]== 0]\n",
    "pos_cov = it_labeled_cov.loc[it_labeled_cov.loc[:,\"pol\"]== 1]\n",
    "neg_cov = it_labeled_cov.loc[it_labeled_cov.loc[:,\"pol\"]== -1]\n",
    "print(\"No. neutral (cov): \", len(neu))\n",
    "print(\"No. positive (cov): \", len(pos))\n",
    "print(\"No. negative (cov): \", len(neg))\n",
    "tot_cov = len(neu_cov) + len(pos_cov) + len(neg_cov)\n",
    "\n",
    "it_neu_cov_labeled, it_preClass_neu_cov_tokens = noiseremoval_and_labelling(neu_cov.loc[:,'text'], 'NEUTRAL')\n",
    "it_pos_cov_labeled, it_preClass_pos_cov_tokens = noiseremoval_and_labelling(pos_cov.loc[:,'text'], 'POSITIVE')\n",
    "it_neg_cov_labeled, it_preClass_neg_cov_tokens = noiseremoval_and_labelling(neg_cov.loc[:,'text'], 'NEGATIVE')\n",
    "\n",
    "it_dataset_cov = it_pos_cov_labeled + it_neg_cov_labeled + it_neu_cov_labeled\n",
    "it_dataset_cov = shuffle(pd.DataFrame(it_dataset_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. features (preclassified):  59240\n",
      "No. features (all):  211\n",
      "No. features (covid):  112\n"
     ]
    }
   ],
   "source": [
    "# split features and labels for SKLEARN classifier\n",
    "\n",
    "features_all = it_dataset_all.loc[:, 0]\n",
    "labels_all  = it_dataset_all.loc[:, 1]\n",
    "features_cov = it_dataset_cov.loc[:, 0]\n",
    "labels_cov  = it_dataset_cov.loc[:, 1]\n",
    "\n",
    "features1 = pd.concat([features_all, features], axis=0)\n",
    "labels1  = pd.concat([labels_all, labels], axis=0)\n",
    "\n",
    "features2 = pd.concat([features_cov, features], axis=0)\n",
    "labels2  = pd.concat([labels_cov, labels], axis=0)\n",
    "\n",
    "print(\"No. features (preclassified): \", len(features))\n",
    "print(\"No. features (all): \", len(features1)-len(features))\n",
    "print(\"No. features (covid): \", len(features2)-len(features))\n",
    "\n",
    "# print(features[:3])\n",
    "# print(features1[:3])\n",
    "# print(features1[249:252])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  211\n",
      "106    NEGATIVE\n",
      "132     NEUTRAL\n",
      "115    NEGATIVE\n",
      "Name: 1, dtype: object\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.67      0.13      0.22        61\n",
      "     NEUTRAL       0.46      0.90      0.61        90\n",
      "    POSITIVE       0.75      0.30      0.43        60\n",
      "\n",
      "    accuracy                           0.51       211\n",
      "   macro avg       0.63      0.44      0.42       211\n",
      "weighted avg       0.60      0.51      0.45       211\n",
      "\n",
      "\n",
      "Accuracy (NB) of manually classified tweets, all: 0.5071090047393365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.57      0.07      0.12        61\n",
      "     NEUTRAL       0.42      0.88      0.57        90\n",
      "    POSITIVE       0.50      0.13      0.21        60\n",
      "\n",
      "    accuracy                           0.43       211\n",
      "   macro avg       0.50      0.36      0.30       211\n",
      "weighted avg       0.49      0.43      0.34       211\n",
      "\n",
      "\n",
      "Accuracy (RF) of manually classified tweets, all: 0.4312796208530806\n"
     ]
    }
   ],
   "source": [
    "## use manually labelled tweets (all) only as test set \n",
    "\n",
    "X1=cv.fit_transform(features1).toarray()\n",
    "y1=labels1\n",
    "# X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 249, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "test_all_index = len(it_dataset_all) # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_test1, X_train1 = X1[:test_all_index], X1[test_all_index:]\n",
    "y_test1, y_train1 = y1[:test_all_index], y1[test_all_index:]\n",
    "print(\"Size (test set): \", len(X_test1))\n",
    "print(y_test1[:3])\n",
    "\n",
    "# sklearn multinomial NB classifier #\n",
    "mnb.fit(X_train1, y_train1)\n",
    "y_pred_NB1 = mnb.predict(X_test1)\n",
    "print (classification_report(y_test1, y_pred_NB1))\n",
    "print(\"\\nAccuracy (NB) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_NB1))\n",
    "\n",
    "# sklearn random forest classfier #\n",
    "RFclassifier.fit(X_train1,y_train1)\n",
    "y_pred_RF1 = RFclassifier.predict(X_test1)\n",
    "print (classification_report(y_test1, y_pred_RF1))\n",
    "print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_RF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  112\n",
      "65    NEUTRAL\n",
      "67    NEUTRAL\n",
      "93    NEUTRAL\n",
      "Name: 1, dtype: object\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.30      0.55      0.39        22\n",
      "     NEUTRAL       0.71      0.44      0.55        68\n",
      "    POSITIVE       0.53      0.73      0.62        22\n",
      "\n",
      "    accuracy                           0.52       112\n",
      "   macro avg       0.52      0.57      0.52       112\n",
      "weighted avg       0.60      0.52      0.53       112\n",
      "\n",
      "\n",
      "Accuracy (NB) of manually classified tweets, all: 0.5178571428571429\n"
     ]
    }
   ],
   "source": [
    "## use manually labelled tweets (covid) only as test set\n",
    "\n",
    "X2=cv.fit_transform(features2).toarray()\n",
    "y2=labels2\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 250, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "test_cov_index = len(it_dataset_cov) # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_test2, X_train2 = X2[:test_cov_index], X2[test_cov_index:]\n",
    "y_test2, y_train2 = y2[:test_cov_index], y2[test_cov_index:]\n",
    "print(\"Size (test set): \", len(X_test2))\n",
    "print(y_test2[:3])\n",
    "\n",
    "# sklearn multinomial NB classifier #\n",
    "mnb.fit(X_train2, y_train2)\n",
    "y_pred_NB2 = mnb.predict(X_test2)\n",
    "print (classification_report(y_test2, y_pred_NB2))\n",
    "print(\"\\nAccuracy (NB) of manually classified tweets, all:\", metrics.accuracy_score(y_test2, y_pred_NB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn random forest classfier #\n",
    "RFclassifier.fit(X_train2, y_train2)\n",
    "y_pred_RF2 = RFclassifier.predict(X_test2)\n",
    "print (classification_report(y_test2, y_pred_RF2))\n",
    "print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test2, y_pred_RF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the classifier for the analysis\n",
    "    NAIVE BAYES USING SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot dynamic classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of days\n",
    "days = ['2020-03-29','2020-03-30','2020-03-31']\n",
    "\n",
    "for y in range(1,31):\n",
    "    nday = str(y)\n",
    "    if len(nday) == 1:\n",
    "        nday = \"0\" + nday\n",
    "    day = \"2020-04-\" +  nday\n",
    "    days.append(day)\n",
    "\n",
    "for y in range(1,12):\n",
    "    nday = str(y)\n",
    "    if len(nday) == 1:\n",
    "        nday = \"0\" + nday\n",
    "    day = \"2020-05-\" +  nday\n",
    "    days.append(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tweets\n",
    "CH_tweets_path = r\"C:\\Users\\barsanti\\Desktop\\Courses\\Computational social media\\project\\CH_dataset\"\n",
    "\n",
    "# Export folder\n",
    "export_path = r\"C:\\Users\\barsanti\\Desktop\\Courses\\Computational social media\\project\\1_descriptive analysis\"\n",
    "\n",
    "df_list = []\n",
    "ref_list = []\n",
    "\n",
    "for day in days:\n",
    "    \n",
    "    # Import the df of tweets as pickle file\n",
    "    with open(CH_tweets_path + \"\\df_tweets_CH_\" + day + \".pkl\", \"rb\") as fp: \n",
    "        data_CH = pickle.load(fp)\n",
    "    with open(CH_tweets_path + \"\\df_ref_tweets_CH_\" + day + \".pkl\", \"rb\") as fp: \n",
    "        data_ref_CH = pickle.load(fp)\n",
    "        \n",
    "    df_list.append(data_CH)\n",
    "    ref_list.append(data_ref_CH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(words, sentence):\n",
    "    for word in words:\n",
    "        if word in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_covidrelated_text(text):\n",
    "    covid_hashtags = [\"COVID2019\", \"COVID-19\", \"CORONAVIRUS\", \"CORONA\", \"VIRUS\", \"AUSGANSPERRE\", \"QUARANTINE\",\n",
    "           \"BEV√ñLKERUNG\", \"MASKEN\", \"STAYHOME\", \"COVID_19\", \"STAYATHOME\", \"CORONAKRISE\",\n",
    "            \"CORONAVIRUSDELCASTIGOMINORE\", \"CORONAINFOCH\", \"COVID\", \"Desinfektion\", \"Spital\",\n",
    "           \"Kranken\", \"infektion\", \"schutz\", \"Restriktionen\", \"BAG_OFSP_UFSP\", \"protectyourselfandothers\",\n",
    "           \"Bundesrat\"]\n",
    "    \n",
    "    melissa_words = [\"covid\", \"covid19\", \"covid2019\", \"covid-19\", \"covid-2019\", \"covid_19\", \"covid_2019\", \"COVID„Éº19\",\n",
    "            \"COVID„Éº2019\", \"corona\", \"virus\", \"coronavirus\", \"coronakrise\", \"coronacrisis\", \"corona-crise\", \n",
    "            \"pandemic\", \"coronapandemic\", \"coronavirusapandemic\", \"covidpandemic\", \"covid19pandemic\", \"covid2019pandemic\" \n",
    "            \"iorestoacasa\", \"forzalombardia\", \"quarantena\", \"covidswitzerland\", \"wtfockdown\", \"stayhome\", \"stayathome\",\n",
    "            \"staysafe\", \"stayhomesavelives\", \"stayhomestaysafe\", \"lockdown\", \"socialdistancing\", \"distancing\", \"quarantine\",\n",
    "            \"quarantinelife\", \"confinement\", \"confinementjour\", \"restezchezvous\", \"lavuedepuismonconfinement\", \n",
    "            \"BloqueoNoSolidaridadSi\", \"protectyourselfandothers\", \"coronavirusdelcastigominore\", \"coronainfoch\", \"masken\",\n",
    "            \"masques\", \"masks\", \"homeoffice\", \"wfh\", \"workfromhome\", \"BAG_OFSP_UFSP\", \"restriktionen\"]\n",
    "    \n",
    "    it_covid_hashtags = ['CoronavirusDelCastigoMinore','coronavirus','COVID19','Covid_19', 'COVID19italia', \n",
    "                     'iorestoacasa', 'andratuttobene', 'pandemia', 'Coronavirus', 'COVID„Éº19', \n",
    "                     'coronavirusitalIa', 'COVID2019italia', 'Covid19', 'versusvirus', 'covid19',\n",
    "                     'coronavirusitalia', 'COVID2019','covid_19italia', 'StayAtHome', 'QuarantineLife']\n",
    "    \n",
    "    covid_hashtags = covid_hashtags + it_covid_hashtags + melissa_words\n",
    "    \n",
    "    covid_hashtags_upper = [k.upper() for k in covid_hashtags]\n",
    "    \n",
    "    sentence = text.upper()\n",
    "    if check(covid_hashtags_upper, sentence):\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "    \n",
    "for n, df in enumerate(df_list):\n",
    "    df_ref = ref_list[n]\n",
    "    for i in df.index:\n",
    "        if df.loc[i,'covid'] == 'no':\n",
    "            df.loc[i,'covid'] = check_covidrelated_text(df_ref.loc[i,'ref_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to collect all the classification info\n",
    "df_classified = pd.DataFrame(0, index = days, columns = [('generic','pos'),\n",
    "                                                         ('generic','neu'),\n",
    "                                                         ('generic','neg'),\n",
    "                                                         ('covid','pos'),\n",
    "                                                         ('covid','neu'),\n",
    "                                                         ('covid','neg')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(generic, pos)</th>\n",
       "      <th>(generic, neu)</th>\n",
       "      <th>(generic, neg)</th>\n",
       "      <th>(covid, pos)</th>\n",
       "      <th>(covid, neu)</th>\n",
       "      <th>(covid, neg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            (generic, pos)  (generic, neu)  (generic, neg)  (covid, pos)  \\\n",
       "2020-03-29               0               0               0             0   \n",
       "2020-03-30               0               0               0             0   \n",
       "2020-03-31               0               0               0             0   \n",
       "2020-04-01               0               0               0             0   \n",
       "2020-04-02               0               0               0             0   \n",
       "2020-04-03               0               0               0             0   \n",
       "2020-04-04               0               0               0             0   \n",
       "2020-04-05               0               0               0             0   \n",
       "2020-04-06               0               0               0             0   \n",
       "2020-04-07               0               0               0             0   \n",
       "2020-04-08               0               0               0             0   \n",
       "2020-04-09               0               0               0             0   \n",
       "2020-04-10               0               0               0             0   \n",
       "2020-04-11               0               0               0             0   \n",
       "2020-04-12               0               0               0             0   \n",
       "2020-04-13               0               0               0             0   \n",
       "2020-04-14               0               0               0             0   \n",
       "2020-04-15               0               0               0             0   \n",
       "2020-04-16               0               0               0             0   \n",
       "2020-04-17               0               0               0             0   \n",
       "2020-04-18               0               0               0             0   \n",
       "2020-04-19               0               0               0             0   \n",
       "2020-04-20               0               0               0             0   \n",
       "2020-04-21               0               0               0             0   \n",
       "2020-04-22               0               0               0             0   \n",
       "2020-04-23               0               0               0             0   \n",
       "2020-04-24               0               0               0             0   \n",
       "2020-04-25               0               0               0             0   \n",
       "2020-04-26               0               0               0             0   \n",
       "2020-04-27               0               0               0             0   \n",
       "2020-04-28               0               0               0             0   \n",
       "2020-04-29               0               0               0             0   \n",
       "2020-04-30               0               0               0             0   \n",
       "2020-05-01               0               0               0             0   \n",
       "2020-05-02               0               0               0             0   \n",
       "2020-05-03               0               0               0             0   \n",
       "2020-05-04               0               0               0             0   \n",
       "2020-05-05               0               0               0             0   \n",
       "2020-05-06               0               0               0             0   \n",
       "2020-05-07               0               0               0             0   \n",
       "2020-05-08               0               0               0             0   \n",
       "2020-05-09               0               0               0             0   \n",
       "2020-05-10               0               0               0             0   \n",
       "2020-05-11               0               0               0             0   \n",
       "\n",
       "            (covid, neu)  (covid, neg)  \n",
       "2020-03-29             0             0  \n",
       "2020-03-30             0             0  \n",
       "2020-03-31             0             0  \n",
       "2020-04-01             0             0  \n",
       "2020-04-02             0             0  \n",
       "2020-04-03             0             0  \n",
       "2020-04-04             0             0  \n",
       "2020-04-05             0             0  \n",
       "2020-04-06             0             0  \n",
       "2020-04-07             0             0  \n",
       "2020-04-08             0             0  \n",
       "2020-04-09             0             0  \n",
       "2020-04-10             0             0  \n",
       "2020-04-11             0             0  \n",
       "2020-04-12             0             0  \n",
       "2020-04-13             0             0  \n",
       "2020-04-14             0             0  \n",
       "2020-04-15             0             0  \n",
       "2020-04-16             0             0  \n",
       "2020-04-17             0             0  \n",
       "2020-04-18             0             0  \n",
       "2020-04-19             0             0  \n",
       "2020-04-20             0             0  \n",
       "2020-04-21             0             0  \n",
       "2020-04-22             0             0  \n",
       "2020-04-23             0             0  \n",
       "2020-04-24             0             0  \n",
       "2020-04-25             0             0  \n",
       "2020-04-26             0             0  \n",
       "2020-04-27             0             0  \n",
       "2020-04-28             0             0  \n",
       "2020-04-29             0             0  \n",
       "2020-04-30             0             0  \n",
       "2020-05-01             0             0  \n",
       "2020-05-02             0             0  \n",
       "2020-05-03             0             0  \n",
       "2020-05-04             0             0  \n",
       "2020-05-05             0             0  \n",
       "2020-05-06             0             0  \n",
       "2020-05-07             0             0  \n",
       "2020-05-08             0             0  \n",
       "2020-05-09             0             0  \n",
       "2020-05-10             0             0  \n",
       "2020-05-11             0             0  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64876,4987) and (4351,3) not aligned: 4987 (dim 1) != 4351 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-5300a6aef4a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnew_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mnew_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[0;32m    738\u001b[0m                 self.class_log_prior_)\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (64876,4987) and (4351,3) not aligned: 4987 (dim 1) != 4351 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Classify all the tweets\n",
    "# Using sklearn mNB classifier\n",
    "for n, day in enumerate(days):\n",
    "    # classify all the tweets\n",
    "    tweets = df_list[n].loc[:,'text']\n",
    "    tokens = noiseremoval(tweets)\n",
    "    tokens = pd.Series(tokens)\n",
    "    new_features = pd.concat([tokens, features], axis=0)\n",
    "    new_X=cv.fit_transform(new_features).toarray()\n",
    "    y_pred = mnb.predict(new_X)\n",
    "    \n",
    "    for i, pol in enumerate(y_pred):\n",
    "        if i < len(tokens):\n",
    "            if pol == 'POSITIVE' and df_list[n].loc[:,'cov'] == 'yes':\n",
    "                df_list[n].loc[i,'pol'] = 1\n",
    "                df_classified.loc[day, '(covid,pos)'] += 1\n",
    "            elif pol == 'POSITIVE' and df_list[n].loc[:,'cov'] == 'no':\n",
    "                df_list[n].loc[i,'pol'] = 1\n",
    "                df_classified.loc[day, '(generic,pos)'] += 1\n",
    "            elif pol == 'NEGATIVE' and df_list[n].loc[:,'cov'] == 'yes':\n",
    "                df_list[n].loc[i,'pol'] = -1\n",
    "                df_classified.loc[day, '(covid,neg)'] += 1            \n",
    "            elif pol == 'NEGATIVE' and df_list[n].loc[:,'cov'] == 'no':\n",
    "                df_list[n].loc[i,'pol'] = -1\n",
    "                df_classified.loc[day, '(generic,neg)'] += 1\n",
    "            elif pol == 'NEUTRAL' and df_list[n].loc[:,'cov'] == 'yes':\n",
    "                df_list[n].loc[i,'pol'] = 0\n",
    "                df_classified.loc[day, '(covid,neu)'] += 1            \n",
    "            elif pol == 'NEUTRAL' and df_list[n].loc[:,'cov'] == 'no':\n",
    "                df_list[n].loc[i,'pol'] = 0\n",
    "                df_classified.loc[day, '(generic,neu)'] += 1\n",
    "\n",
    "df_classified.head()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12, 6))\n",
    "plt.bar(days,df_classified)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.title('Polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEUTRAL', 'NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on a single sentence\n",
    "sentence = ['non sono triste perch√® mi √® morto il arcotangente alla cosecante di alfa','sono triste perch√® mi √® morto il gatto']\n",
    "tokens = noiseremoval(sentence)\n",
    "tokens = pd.Series(tokens)\n",
    "new_features = pd.concat([tokens, features], axis=0)\n",
    "\n",
    "new_X=cv.fit_transform(new_features).toarray()\n",
    "\n",
    "y_pred = mnb.predict(new_X)\n",
    "y_pred[:len(tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_list[0].loc[:,'text']\n",
    "tokens = noiseremoval(tweets)\n",
    "tokens = pd.Series(tokens)\n",
    "new_features = pd.concat([tokens, features], axis=0)\n",
    "new_X=cv.fit_transform(new_features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64876,4987) and (4351,3) not aligned: 4987 (dim 1) != 4351 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-526480dfafb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[0;32m    738\u001b[0m                 self.class_log_prior_)\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (64876,4987) and (4351,3) not aligned: 4987 (dim 1) != 4351 (dim 0)"
     ]
    }
   ],
   "source": [
    "y_pred = mnb.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
