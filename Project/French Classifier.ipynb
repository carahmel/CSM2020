{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tweets = []\n",
    "tmin = 500\n",
    "tmax = 479500 # latest file name\n",
    "\n",
    "for i in range(tmin, tmax + 1, 500):\n",
    "    file_name = \"../dataKA/all_tweets/tweets\" + str(i)\n",
    "    file_object = open(file_name, 'rb')\n",
    "    for tweet in pickle.load(file_object):\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"covid\", \"covid19\", \"covid2019\", \"covid-19\", \"covid-2019\", \"covid_19\", \"covid_2019\", \"COVIDー19\",\n",
    "            \"COVIDー2019\", \"corona\", \"virus\", \"coronavirus\", \"coronakrise\", \"coronacrisis\", \"corona-crise\", \n",
    "            \"pandemic\", \"coronapandemic\", \"coronavirusapandemic\", \"covidpandemic\", \"covid19pandemic\", \"covid2019pandemic\" \n",
    "            \"iorestoacasa\", \"forzalombardia\", \"quarantena\", \"covidswitzerland\", \"wtfockdown\", \"stayhome\", \"stayathome\",\n",
    "            \"staysafe\", \"stayhomesavelives\", \"stayhomestaysafe\", \"lockdown\", \"socialdistancing\", \"distancing\", \"quarantine\",\n",
    "            \"quarantinelife\", \"confinement\", \"confinementjour\", \"restezchezvous\", \"lavuedepuismonconfinement\", \n",
    "            \"BloqueoNoSolidaridadSi\", \"protectyourselfandothers\", \"coronavirusdelcastigominore\", \"coronainfoch\", \"masken\",\n",
    "            \"masques\", \"masks\", \"homeoffice\", \"wfh\", \"workfromhome\", \"BAG_OFSP_UFSP\", \"restriktionen\"] \n",
    "\n",
    "\n",
    "# from Karolos\n",
    "def check(words, sentence):\n",
    "    for word in words:\n",
    "        if word in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "keywords_lower = [k.lower() for k in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. FR covid tweets:  7485\n"
     ]
    }
   ],
   "source": [
    "# COVID-related\n",
    "fr_cov_date = []\n",
    "fr_cov_tid = []\n",
    "fr_cov_text = []\n",
    "fr_cov_tweets = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    if check(keywords_lower, tweet[\"text\"].lower()):\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt = cnt + 1\n",
    "            fr_cov_tweets.append(tweet)\n",
    "            fr_cov_tid.append(tweet[\"id\"])\n",
    "            fr_cov_date.append(tweet[\"created_at\"])\n",
    "            try:\n",
    "                fr_cov_text.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            except KeyError:\n",
    "                fr_cov_text.append(tweet[\"text\"])\n",
    "            \n",
    "    elif \"extended_tweet\" in tweet and check(keywords_lower, tweet[\"extended_tweet\"][\"full_text\"]):\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt = cnt + 1\n",
    "            fr_cov_tweets.append(tweet)\n",
    "            fr_cov_tid.append(tweet[\"id\"])\n",
    "            fr_cov_date.append(tweet[\"created_at\"])\n",
    "            fr_cov_text.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "            \n",
    "    elif \"retweeted_status\" in tweet and check(keywords_lower, tweet[\"retweeted_status\"][\"full_text\"]):\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt = cnt + 1\n",
    "            fr_cov_tweets.append(tweet)\n",
    "            fr_cov_tid.append(tweet[\"id\"])\n",
    "            fr_cov_date.append(tweet[\"created_at\"])\n",
    "            fr_cov_text.append(tweet[\"retweeted_status\"][\"full_text\"])\n",
    "            \n",
    "    elif \"quoted_status\" in tweet and check(keywords_lower, tweet[\"quoted_status\"][\"text\"]):\n",
    "        if tweet[\"lang\"] == \"fr\":\n",
    "            cnt = cnt + 1\n",
    "            fr_cov_tweets.append(tweet)\n",
    "            fr_cov_tid.append(tweet[\"id\"])\n",
    "            fr_cov_date.append(tweet[\"created_at\"])\n",
    "            fr_cov_text.append(tweet[\"quoted_status\"][\"text\"])\n",
    "\n",
    "print(\"No. FR covid tweets: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. FR tweets (total):  107978\n"
     ]
    }
   ],
   "source": [
    "# All FR tweets\n",
    "fr_all_date = []\n",
    "fr_all_tid = []\n",
    "fr_all_text = []\n",
    "fr_all_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    if tweet['lang'] == 'fr':\n",
    "        fr_all_tweets.append(tweet)\n",
    "        fr_all_tid.append(tweet[\"id\"])\n",
    "        fr_all_date.append(tweet[\"created_at\"])\n",
    "        try:\n",
    "            fr_all_text.append(tweet[\"extended_tweet\"][\"full_text\"])\n",
    "        except KeyError:\n",
    "            fr_all_text.append(tweet[\"text\"])\n",
    "\n",
    "print(\"No. FR tweets (total): \", len(fr_all_tid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## Export full tweet datasets ##\n",
    "\n",
    "fr_all = pd.concat([pd.Series(fr_all_tid), \n",
    "                           pd.Series(fr_all_date), \n",
    "                           pd.Series(fr_all_text)],\n",
    "                          axis = 1)\n",
    "fr_covid = pd.concat([pd.Series(fr_cov_tid),\n",
    "                             pd.Series(fr_cov_date),\n",
    "                             pd.Series(fr_cov_text)],\n",
    "                            axis = 1)\n",
    "\n",
    "# fr_all.to_csv(\"../dataKA/latest/fr_tweets_all.csv\", encoding = \"utf-8\")\n",
    "# fr_covid.to_csv(\"../dataKA/latest/fr_tweets_covid.csv\", encoding = \"utf-8\")\n",
    "\n",
    "## Export text of covid dataset and full dataset for tool training ## \n",
    "out_fr_all_text = pd.DataFrame(fr_all_text)\n",
    "out_fr_cov_text = pd.DataFrame(fr_cov_text)\n",
    "\n",
    "# out_fr_all_text.to_csv(\"../dataKA/latest/fr_alltext.csv\", encoding = \"utf-8\")\n",
    "# out_fr_cov_text.to_csv(\"../dataKA/latest/fr_covidtext.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#### Define pre-processing subfunctions ####\n",
    "\n",
    "def removeNonAlphaNumChars(tweets):\n",
    "    tweets = [re.sub(r'\\-', r' ', tweet) for tweet in tweets]  # replace - with SPACE\n",
    "    tweets = [re.sub(r'[^\\w\\s]', r'', tweet) for tweet in tweets]  #remove other NON alpha numeric, excluding whitespace\n",
    "    return tweets\n",
    "\n",
    "def replaceContractions(tweets):\n",
    "    tweets = [re.sub(r\"[’\\']\", r' ', tweet) for tweet in tweets]  ## !!!! WEIRD APOSTROPHE BEWARE !!!\n",
    "    return tweets\n",
    "\n",
    "# FROM DEFFRO: https://github.com/Deffro/text-preprocessing-techniques\n",
    "def removeUnicode(tweets):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    tweets = [re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'[^\\x00-\\x7f]',r'',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceURL(tweets):\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    tweets = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def replaceAtUser(tweets):\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    tweets = [re.sub('@[^\\s]+','atUser',tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def removeHashtagInFrontOfWord(tweets):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    tweets = [re.sub(r'#([^\\s]+)', r'\\1', tweet) for tweet in tweets]\n",
    "    return tweets\n",
    "# end DEFFRO\n",
    "\n",
    "\n",
    "#### DEFINE MAIN PREPROCESSING FUNCTIONS ####\n",
    "\n",
    "def cleanTweets(tweets):\n",
    "    tweets = [tweet.lower() for tweet in tweets]  #convert to lowercase\n",
    "    tweets = replaceURL(tweets) #replace URLs\n",
    "    tweets = replaceAtUser(tweets)  # replace @user\n",
    "    tweets = removeHashtagInFrontOfWord(tweets)  # remove hashtag\n",
    "    tweets = replaceContractions(tweets) # replace ' contractions with space (e.g. l'ordre = l ordre)\n",
    "    tweets = removeNonAlphaNumChars(tweets)  # remove non alphanumeric characters\n",
    "    return tweets\n",
    "\n",
    "def tokenizeTweets(tweets):\n",
    "    tokens = [nltk.word_tokenize(tweet) for tweet in tweets]\n",
    "    return tokens\n",
    "\n",
    "def applyStoplist(tokens):\n",
    "    tokens = [[word for word in token if len(word) > 2 and (word not in stoplist)] for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def stemTokens(tokens):\n",
    "    tokens = [[stemmer.stem(word) for word in token] for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral:  20000\n",
      "No. positive:  14328\n",
      "No. negative:  2358\n"
     ]
    }
   ],
   "source": [
    "# Import preclassified EN tweets (football)\n",
    "fr_preClass = pd.read_csv(\"C:/Users/pang/Desktop/Stuff/_PhD/PhD Courses/_CSM/Project/preClassified/FR_3cat_worldcup.csv\")\n",
    "# fr_preClass = shuffle(fr_preClass)\n",
    "\n",
    "neutral = fr_preClass[fr_preClass[\"sentiment\"]==\"NEUTRAL\"]\n",
    "positive = fr_preClass[fr_preClass[\"sentiment\"]==\"POSITIVE\"]\n",
    "negative = fr_preClass[fr_preClass[\"sentiment\"]==\"NEGATIVE\"]\n",
    "\n",
    "neutral = shuffle(neutral)[:20000]\n",
    "\n",
    "print(\"No. neutral: \", len(neutral))\n",
    "print(\"No. positive: \", len(positive))\n",
    "print(\"No. negative: \", len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start tweet preprocessing ##\n",
    "stoplist = stopwords.words('french')\n",
    "other_stopwords = \"atUser URL\"\n",
    "stoplist = stoplist + other_stopwords.split() # final list of stopwords\n",
    "# lemmatizer = WordNetLemmatizer() # only for EN\n",
    "stemmer = SnowballStemmer('french') # set stemmer\n",
    "\n",
    "fr_preClass_pos_clean = cleanTweets(positive[\"tweet_text\"])\n",
    "fr_preClass_pos_tokens = tokenizeTweets(fr_preClass_pos_clean)\n",
    "fr_preClass_pos_tokens = applyStoplist(fr_preClass_pos_tokens)\n",
    "fr_preClass_pos_tokens = stemTokens(fr_preClass_pos_tokens)\n",
    "\n",
    "fr_preClass_neg_clean = cleanTweets(negative[\"tweet_text\"])\n",
    "fr_preClass_neg_tokens = tokenizeTweets(fr_preClass_neg_clean)\n",
    "fr_preClass_neg_tokens = applyStoplist(fr_preClass_neg_tokens)\n",
    "fr_preClass_neg_tokens = stemTokens(fr_preClass_neg_tokens)\n",
    "\n",
    "fr_preClass_neu_clean = cleanTweets(neutral[\"tweet_text\"])\n",
    "fr_preClass_neu_tokens = tokenizeTweets(fr_preClass_neu_clean)\n",
    "fr_preClass_neu_tokens = applyStoplist(fr_preClass_neu_tokens)\n",
    "fr_preClass_neu_tokens = stemTokens(fr_preClass_neu_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_pos_labeled = [[token, \"Positive\"] for token in fr_preClass_pos_tokens]\n",
    "fr_neg_labeled = [[token, \"Negative\"] for token in fr_preClass_neg_tokens]\n",
    "fr_neu_labeled = [[token, \"Neutral\"] for token in fr_preClass_neu_tokens]\n",
    "\n",
    "fr_dataset = fr_pos_labeled + fr_neg_labeled + fr_neu_labeled\n",
    "fr_dataset = shuffle(pd.DataFrame(fr_dataset))\n",
    "\n",
    "# split features and labels for SKLEARN classifier\n",
    "features = fr_dataset.loc[:, 0]\n",
    "labels  = fr_dataset.loc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in-domain test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare for classifier training (SKLEARN) ##\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=dummy,preprocessor=dummy,max_df=0.9, min_df=10)\n",
    "X=cv.fit_transform(features).toarray()\n",
    "y=labels\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAIVE BAYES USING SKLEARN ##\n",
    "\n",
    "#Import Multinomial Naive Bayes model (https://towardsdatascience.com/sentiment-analysis-of-tweets-using-multinomial-naive-bayes-1009ed24276b)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a multinomial NB Classifier\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_NB = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## RANDOM FOREST USING SKLEARN ##\n",
    "\n",
    "# # Fitting classifier to the Training set\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# RFclassifier=RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "# RFclassifier.fit(X_train,y_train)\n",
    "# # Predicting the Test set results\n",
    "# y_pred_RF = RFclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.34      0.58      0.43       736\n",
      "     Neutral       0.76      0.63      0.68      5982\n",
      "    Positive       0.64      0.72      0.68      4288\n",
      "\n",
      "    accuracy                           0.66     11006\n",
      "   macro avg       0.58      0.64      0.60     11006\n",
      "weighted avg       0.68      0.66      0.67     11006\n",
      "\n",
      "\n",
      "Accuracy (NB): 0.6597310557877522\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y_test, y_pred_NB))\n",
    "print(\"\\nAccuracy (NB):\", metrics.accuracy_score(y_test, y_pred_NB))\n",
    "\n",
    "# print (\"\\n\", classification_report(y_test, y_pred_RF))\n",
    "# print(\"\\nAccuracy (RF):\", metrics.accuracy_score(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use collected tweets as test set instead of in-domain tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. neutral (all):  371\n",
      "No. positive (all):  75\n",
      "No. negative (all):  54\n",
      "No. neutral (covid):  414\n",
      "No. positive (covid):  44\n",
      "No. negative (covid):  42\n",
      "No. features (preclassified):  36686\n",
      "No. features (all):  500\n",
      "No. features (covid):  500\n"
     ]
    }
   ],
   "source": [
    "### load manually classified tweets (CH/Covid) ###\n",
    "\n",
    "fr_labeled_all = pd.read_csv(\"../dataKA/frlabeledAll1.csv\")\n",
    "fr_labeled_cov = pd.read_csv(\"../dataKA/frlabeledCovid1.csv\")\n",
    "\n",
    "## ALL ##\n",
    "neu = fr_labeled_all[fr_labeled_all[\"label\"]==\"neu\"]\n",
    "pos = fr_labeled_all[fr_labeled_all[\"label\"]==\"pos\"]\n",
    "neg = fr_labeled_all[fr_labeled_all[\"label\"]==\"neg\"]\n",
    "print(\"No. neutral (all): \", len(neu))\n",
    "print(\"No. positive (all): \", len(pos))\n",
    "print(\"No. negative (all): \", len(neg))\n",
    "\n",
    "# Start preprocessing #\n",
    "fr_labeled_all_pos_clean = cleanTweets(pos[\"text\"])\n",
    "fr_labeled_all_pos_tokens = tokenizeTweets(fr_labeled_all_pos_clean)\n",
    "fr_labeled_all_pos_tokens = applyStoplist(fr_labeled_all_pos_tokens)\n",
    "fr_labeled_all_pos_tokens = stemTokens(fr_labeled_all_pos_tokens)\n",
    "\n",
    "fr_labeled_all_neg_clean = cleanTweets(neg[\"text\"])\n",
    "fr_labeled_all_neg_tokens = tokenizeTweets(fr_labeled_all_neg_clean)\n",
    "fr_labeled_all_neg_tokens = applyStoplist(fr_labeled_all_neg_tokens)\n",
    "fr_labeled_all_neg_tokens = stemTokens(fr_labeled_all_neg_tokens)\n",
    "\n",
    "fr_labeled_all_neu_clean = cleanTweets(neu[\"text\"])\n",
    "fr_labeled_all_neu_tokens = tokenizeTweets(fr_labeled_all_neu_clean)\n",
    "fr_labeled_all_neu_tokens = applyStoplist(fr_labeled_all_neu_tokens)\n",
    "fr_labeled_all_neu_tokens = stemTokens(fr_labeled_all_neu_tokens)\n",
    "\n",
    "fr_pos_labeled_all = [[token, \"Positive\"] for token in fr_labeled_all_pos_tokens]\n",
    "fr_neg_labeled_all = [[token, \"Negative\"] for token in fr_labeled_all_neg_tokens]\n",
    "fr_neu_labeled_all = [[token, \"Neutral\"] for token in fr_labeled_all_neu_tokens]\n",
    "\n",
    "fr_dataset_all = fr_pos_labeled_all + fr_neg_labeled_all + fr_neu_labeled_all\n",
    "fr_dataset_all = shuffle(pd.DataFrame(fr_dataset_all))\n",
    "\n",
    "\n",
    "## COVID ##\n",
    "neu_cov = fr_labeled_cov[fr_labeled_cov[\"label\"]==\"neu\"]\n",
    "pos_cov = fr_labeled_cov[fr_labeled_cov[\"label\"]==\"pos\"]\n",
    "neg_cov = fr_labeled_cov[fr_labeled_cov[\"label\"]==\"neg\"]\n",
    "print(\"No. neutral (covid): \", len(neu_cov))\n",
    "print(\"No. positive (covid): \", len(pos_cov))\n",
    "print(\"No. negative (covid): \", len(neg_cov))\n",
    "\n",
    "## Start preprocessing ##\n",
    "fr_labeled_cov_pos_clean = cleanTweets(pos_cov[\"text\"])\n",
    "fr_labeled_cov_pos_tokens = tokenizeTweets(fr_labeled_cov_pos_clean)\n",
    "fr_labeled_cov_pos_tokens = applyStoplist(fr_labeled_cov_pos_tokens)\n",
    "fr_labeled_cov_pos_tokens = stemTokens(fr_labeled_cov_pos_tokens)\n",
    "\n",
    "fr_labeled_cov_neg_clean = cleanTweets(neg_cov[\"text\"])\n",
    "fr_labeled_cov_neg_tokens = tokenizeTweets(fr_labeled_cov_neg_clean)\n",
    "fr_labeled_cov_neg_tokens = applyStoplist(fr_labeled_cov_neg_tokens)\n",
    "fr_labeled_cov_neg_tokens = stemTokens(fr_labeled_cov_neg_tokens)\n",
    "\n",
    "fr_labeled_cov_neu_clean = cleanTweets(neu_cov[\"text\"])\n",
    "fr_labeled_cov_neu_tokens = tokenizeTweets(fr_labeled_cov_neu_clean)\n",
    "fr_labeled_cov_neu_tokens = applyStoplist(fr_labeled_cov_neu_tokens)\n",
    "fr_labeled_cov_neu_tokens = stemTokens(fr_labeled_cov_neu_tokens)\n",
    "\n",
    "fr_pos_labeled_cov = [[token, \"Positive\"] for token in fr_labeled_cov_pos_tokens]\n",
    "fr_neg_labeled_cov = [[token, \"Negative\"] for token in fr_labeled_cov_neg_tokens]\n",
    "fr_neu_labeled_cov = [[token, \"Neutral\"] for token in fr_labeled_cov_neu_tokens]\n",
    "\n",
    "fr_dataset_cov = fr_pos_labeled_cov + fr_neg_labeled_cov + fr_neu_labeled_cov\n",
    "fr_dataset_cov = shuffle(pd.DataFrame(fr_dataset_cov))\n",
    "\n",
    "\n",
    "## split features and labels for SKLEARN classifier\n",
    "features_all = fr_dataset_all.loc[:, 0]\n",
    "labels_all  = fr_dataset_all.loc[:, 1]\n",
    "features_cov = fr_dataset_cov.loc[:, 0]\n",
    "labels_cov  = fr_dataset_cov.loc[:, 1]\n",
    "\n",
    "features1 = pd.concat([features_all, features], axis=0)\n",
    "labels1  = pd.concat([labels_all, labels], axis=0)\n",
    "\n",
    "features2 = pd.concat([features_cov, features], axis=0)\n",
    "labels2  = pd.concat([labels_cov, labels], axis=0)\n",
    "\n",
    "print(\"No. features (preclassified): \", len(features))\n",
    "print(\"No. features (all): \", len(features1)-len(features))\n",
    "print(\"No. features (covid): \", len(features2)-len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  500\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.27      0.50      0.35        54\n",
      "     Neutral       0.83      0.63      0.71       371\n",
      "    Positive       0.27      0.44      0.34        75\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.46      0.52      0.47       500\n",
      "weighted avg       0.69      0.58      0.62       500\n",
      "\n",
      "\n",
      "Accuracy (NB) of manually classified tweets, all: 0.584\n"
     ]
    }
   ],
   "source": [
    "## use manually labelled tweets (ALL) only as test set ##\n",
    "\n",
    "X1=cv.fit_transform(features1).toarray()\n",
    "y1=labels1\n",
    "# X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 249, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "test_all_index = 500 # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT (first 500 are collected tweets)\n",
    "X_test1, X_train1 = X1[:test_all_index], X1[test_all_index:]\n",
    "y_test1, y_train1 = y1[:test_all_index], y1[test_all_index:]\n",
    "print(\"Size (test set): \", len(X_test1))\n",
    "\n",
    "# sklearn multinomial NB classifier #\n",
    "mnb.fit(X_train1, y_train1)\n",
    "y_pred_NB1 = mnb.predict(X_test1)\n",
    "print (\"\\n\", classification_report(y_test1, y_pred_NB1))\n",
    "print(\"\\nAccuracy (NB) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_NB1))\n",
    "\n",
    "# # sklearn random forest classfier #\n",
    "# RFclassifier.fit(X_train1,y_train1)\n",
    "# y_pred_RF1 = RFclassifier.predict(X_test1)\n",
    "# print (\"\\n\", classification_report(y_test1, y_pred_RF1))\n",
    "# print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test1, y_pred_RF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  500\n",
      "497     Neutral\n",
      "143     Neutral\n",
      "52     Negative\n",
      "Name: 1, dtype: object\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.13      0.74      0.22        42\n",
      "     Neutral       0.90      0.41      0.56       414\n",
      "    Positive       0.36      0.61      0.46        44\n",
      "\n",
      "    accuracy                           0.46       500\n",
      "   macro avg       0.47      0.59      0.41       500\n",
      "weighted avg       0.79      0.46      0.53       500\n",
      "\n",
      "\n",
      "Accuracy (NB) of manually classified tweets, covid-related: 0.456\n"
     ]
    }
   ],
   "source": [
    "## use manually labelled tweets (COVID) only as test set ##\n",
    "\n",
    "X2=cv.fit_transform(features2).toarray()\n",
    "y2=labels2\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 250, shuffle = \"FALSE\", stratify=None)\n",
    "\n",
    "test_cov_index = 500 # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_test2, X_train2 = X2[:test_cov_index], X2[test_cov_index:]\n",
    "y_test2, y_train2 = y2[:test_cov_index], y2[test_cov_index:]\n",
    "print(\"Size (test set): \", len(X_test2))\n",
    "print(y_test2[:3])\n",
    "\n",
    "# sklearn multinomial NB classifier #\n",
    "mnb.fit(X_train2, y_train2)\n",
    "y_pred_NB2 = mnb.predict(X_test2)\n",
    "print (\"\\n\", classification_report(y_test2, y_pred_NB2))\n",
    "print(\"\\nAccuracy (NB) of manually classified tweets, covid-related:\", metrics.accuracy_score(y_test2, y_pred_NB2))\n",
    "\n",
    "# # sklearn random forest classfier #\n",
    "# RFclassifier.fit(X_train2, y_train2)\n",
    "# y_pred_RF2 = RFclassifier.predict(X_test2)\n",
    "# print (\"\\n\", classification_report(y_test2, y_pred_RF2))\n",
    "# print(\"\\nAccuracy (RF) of manually classified tweets, all:\", metrics.accuracy_score(y_test2, y_pred_RF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply trained tool to predict polarity of collected FR tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (test set):  107978\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn mNB classifier\n",
    "labels_all = pd.Series([\"tbc\"] * len(fr_all_tweets))\n",
    "features_all = cleanTweets(fr_all_text)\n",
    "features_all = tokenizeTweets(features_all)\n",
    "features_all = applyStoplist(features_all)\n",
    "features_all = stemTokens(features_all)\n",
    "features_all = pd.Series(features_all)\n",
    "labels_train = labels\n",
    "features_train = features\n",
    "\n",
    "X_all = cv.fit_transform(pd.concat([features_train, features_all], axis=0)).toarray()\n",
    "y_all = pd.concat([labels_train, labels_train], axis=0)\n",
    "\n",
    "cv.fit_transform(features).toarray()\n",
    "train_index = len(features_train) # USE THIS INSTEAD OF SKLEARN HELPER TO ENSURE CORRECT SPLIT\n",
    "X_all_train, X_all_test = X_all[:train_index], X_all[train_index:]\n",
    "y_all_train, y_all_test = y_all[:train_index], y_all[train_index:]\n",
    "\n",
    "print(\"Size (test set): \", len(X_all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. predictions:  107978\n",
      "['Positive' 'Positive' 'Neutral' 'Positive' 'Neutral' 'Neutral' 'Negative'\n",
      " 'Neutral' 'Neutral' 'Negative']\n",
      "                     0                               1  \\\n",
      "0  1244238235351556098  Sun Mar 29 12:21:26 +0000 2020   \n",
      "1  1244238255916228608  Sun Mar 29 12:21:31 +0000 2020   \n",
      "2  1244238328943259649  Sun Mar 29 12:21:49 +0000 2020   \n",
      "3  1244238523642847234  Sun Mar 29 12:22:35 +0000 2020   \n",
      "4  1244238528025964544  Sun Mar 29 12:22:36 +0000 2020   \n",
      "\n",
      "                                                   2         3  \n",
      "0                           @clemd95 Serieux ?! 😱😱😱😱  Positive  \n",
      "1  Mirabelle incoming! 😋 @ Baume-les-Dames https:...  Positive  \n",
      "2                                @AkaDiane09 Fume le   Neutral  \n",
      "3  🇪🇺 L'Europe des solidarités.🇪🇺\\n\\nMerci aux Al...  Positive  \n",
      "4  @charlotteepenex @AlexGubser @BaselStadt @Etat...   Neutral  \n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "mnb.fit(X_all_train, y_all_train)\n",
    "y_all_pred_NB = mnb.predict(X_all_test)\n",
    "fr_res_all_NB = pd.concat([pd.Series(fr_all_tid), pd.Series(fr_all_date),\n",
    "                     pd.Series(fr_all_text), pd.Series(y_all_pred_NB)], \n",
    "                    axis = 1) \n",
    "fr_res_all_NB.to_csv(\"../dataKA/latest/fr_res_all_nb.csv\", encoding = \"utf-8\")\n",
    "print(\"No. predictions: \", len(y_all_pred_NB))\n",
    "print(y_all_pred_NB[:10])\n",
    "print(fr_res_all_NB[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random forest\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# RFclassifier=RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "# RFclassifier.fit(X_all_train, y_all_train)\n",
    "# y_all_pred_RF2 = RFclassifier.predict(X_all_test)\n",
    "# res_all_RF = pd.concat([pd.Series(fr_all_tweet_id), pd.Series(fr_all_date), \n",
    "#                      pd.Series(fr_all_coord), pd.Series(fr_all_place), \n",
    "#                      pd.Series(fr_all_text), pd.Series(y_all_pred_RF2)], \n",
    "#                     axis = 1) \n",
    "# res_all_RF.to_csv(\"../dataKA/res_all_FR_rf.csv\", encoding = \"utf-8\")\n",
    "# print(\"No. predictions: \", len(y_all_pred_RF2))\n",
    "# print(res_all_RF[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
